{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face2BMI: Complete End-to-End Pipeline\n",
    "### From Data Preprocessing to Model Training, Validation & Ablation Study\n",
    "\n",
    "This notebook provides a comprehensive pipeline for BMI prediction from facial images.\n",
    "\n",
    "**Pipeline Overview:**\n",
    "1. **Data Preprocessing**: Background removal and face segmentation using MediaPipe\n",
    "2. **Feature Extraction**: 468 facial landmarks detection + 7 geometric features\n",
    "3. **Data Merging**: Combine geometric features with BMI labels\n",
    "4. **Model Training**: Multiple architectures (CNN, Hybrid, GNN)\n",
    "5. **K-Fold Cross-Validation**: Robust evaluation with 5 folds\n",
    "6. **Comprehensive Evaluation**: R¬≤, MAE, RMSE, MAPE metrics\n",
    "7. **Ablation Study**: Component-wise model analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Part 1: Data Preprocessing & Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install mediapipe -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T03:17:53.647819Z",
     "iopub.status.busy": "2025-10-27T03:17:53.647577Z",
     "iopub.status.idle": "2025-10-27T03:18:06.050045Z",
     "shell.execute_reply": "2025-10-27T03:18:06.049254Z",
     "shell.execute_reply.started": "2025-10-27T03:17:53.647799Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 03:17:55.193890: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761535075.372003      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761535075.426444      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è No images found in /kaggle/working/upscaling_image\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import mediapipe as mp\n",
    "\n",
    "# ==== Paths ====\n",
    "input_folder = \"/kaggle/input/morph/Dataset/Images/Train\"\n",
    "output_folder = \"/kaggle/working/ROI\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# ==== Collect image paths ====\n",
    "image_paths = glob(os.path.join(input_folder, \"*.jpg\")) \n",
    "\n",
    "if len(image_paths) == 0:\n",
    "    print(f\"‚ö†Ô∏è No images found in {input_folder}\")\n",
    "else:\n",
    "    # ==== Initialize MediaPipe ====\n",
    "    mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "    selfie_segmentation = mp_selfie_segmentation.SelfieSegmentation(model_selection=1)\n",
    "\n",
    "    for path in tqdm(image_paths, desc=\"Removing background\", ncols=80):\n",
    "        try:\n",
    "            img = cv2.imread(path)\n",
    "            if img is None or img.size == 0:\n",
    "                continue\n",
    "\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            results = selfie_segmentation.process(img_rgb)\n",
    "\n",
    "            # Create mask and apply\n",
    "            mask = results.segmentation_mask\n",
    "            condition = mask > 0.5  # keep only face region\n",
    "            bg = np.zeros_like(img, dtype=np.uint8)  # black background\n",
    "            output = np.where(condition[..., None], img, bg)\n",
    "\n",
    "            # Optional: Crop to tight face region\n",
    "            gray = cv2.cvtColor(output, cv2.COLOR_BGR2GRAY)\n",
    "            coords = cv2.findNonZero(gray)\n",
    "            if coords is not None:\n",
    "                x, y, w, h = cv2.boundingRect(coords)\n",
    "                output = output[y:y+h, x:x+w]\n",
    "\n",
    "            # Save\n",
    "            save_path = os.path.join(output_folder, os.path.basename(path))\n",
    "            cv2.imwrite(save_path, output)\n",
    "\n",
    "        except:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T03:18:06.700741Z",
     "iopub.status.busy": "2025-10-27T03:18:06.700430Z",
     "iopub.status.idle": "2025-10-27T03:18:06.954134Z",
     "shell.execute_reply": "2025-10-27T03:18:06.953200Z",
     "shell.execute_reply.started": "2025-10-27T03:18:06.700719Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# ==== Path ====\n",
    "roi_folder = \"/kaggle/working/ROI\"\n",
    "\n",
    "# ==== Collect first 10 images ====\n",
    "image_paths = glob(os.path.join(roi_folder, \"*.jpg\")) \n",
    "\n",
    "image_paths = image_paths[:10]\n",
    "\n",
    "# ==== Display ====\n",
    "plt.figure(figsize=(15, 6))\n",
    "for i, path in enumerate(image_paths):\n",
    "    img = cv2.imread(path)\n",
    "    if img is not None:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(os.path.basename(path))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==== Paths ====\n",
    "input_folder = \"/kaggle/working/ROI\"\n",
    "output_folder = \"/kaggle/working/face_mesh_output\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# ==== Collect images ====\n",
    "image_paths = glob(os.path.join(input_folder, \"*.jpg\")) \n",
    "\n",
    "print(f\"üìÅ Total images found: {len(image_paths)}\")\n",
    "\n",
    "if len(image_paths) == 0:\n",
    "    raise ValueError(\"‚ùå No images found in the directory. Please check your input path.\")\n",
    "\n",
    "# ==== Mediapipe Setup ====\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True,\n",
    "                                  max_num_faces=1,\n",
    "                                  refine_landmarks=True,\n",
    "                                  min_detection_confidence=0.5)\n",
    "\n",
    "# ==== Processing ====\n",
    "processed_images = []\n",
    "for path in tqdm(image_paths, desc=\"Drawing landmarks\", ncols=80):\n",
    "    img = cv2.imread(path)\n",
    "    if img is None:\n",
    "        continue\n",
    "    \n",
    "    h, w, _ = img.shape\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(img_rgb)\n",
    "    \n",
    "    if not results.multi_face_landmarks:\n",
    "        continue\n",
    "\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "        for idx, lm in enumerate(face_landmarks.landmark):\n",
    "            x, y = int(lm.x * w), int(lm.y * h)\n",
    "           \n",
    "            cv2.circle(img, (x, y), 1, (0, 255, 0), -1)\n",
    "            # Draw index number in green\n",
    "            cv2.putText(img, str(idx), (x, y), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        0.25, (0, 0, 250), 1, cv2.LINE_AA)\n",
    "    \n",
    "    # Save output image\n",
    "    save_path = os.path.join(output_folder, os.path.basename(path))\n",
    "    cv2.imwrite(save_path, img)\n",
    "    \n",
    "    # Collect few images for display\n",
    "    if len(processed_images) < 10:\n",
    "        processed_images.append(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "print(f\"\\n‚úÖ FaceMesh landmarks drawn and saved in: {output_folder}\")\n",
    "\n",
    "# ==== Show sample outputs ====\n",
    "if processed_images:\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    for i, img in enumerate(processed_images):\n",
    "        plt.subplot(2, 5, i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Image {i+1}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No landmarks detected in any image.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-27T03:18:06.954788Z",
     "iopub.status.idle": "2025-10-27T03:18:06.955091Z",
     "shell.execute_reply": "2025-10-27T03:18:06.954947Z",
     "shell.execute_reply.started": "2025-10-27T03:18:06.954933Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "\n",
    "# ==== Paths ====\n",
    "input_folder = \"/kaggle/working/ROI\"\n",
    "output_folder = \"/kaggle/working/face_mesh_output\"\n",
    "csv_path = \"/kaggle/working/features.csv\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# ==== Collect images ====\n",
    "image_paths = glob(os.path.join(input_folder, \"*.jpg\")) + \\\n",
    "              glob(os.path.join(input_folder, \"*.jpeg\")) + \\\n",
    "              glob(os.path.join(input_folder, \"*.png\"))\n",
    "\n",
    "print(f\"üìÅ Total images found: {len(image_paths)}\")\n",
    "\n",
    "if len(image_paths) == 0:\n",
    "    raise ValueError(\"‚ùå No images found in the directory. Please check your input path.\")\n",
    "\n",
    "# ==== Mediapipe Setup ====\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True,\n",
    "                                  max_num_faces=1,\n",
    "                                  refine_landmarks=True,\n",
    "                                  min_detection_confidence=0.5)\n",
    "\n",
    "# ==== Function to Calculate Features ====\n",
    "def calculate_geometric_features(landmarks):\n",
    "    if landmarks is None or len(landmarks) < 468:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        features = {}\n",
    "        # Key landmarks\n",
    "        chin = landmarks[152]\n",
    "        forehead_center = landmarks[10]\n",
    "        left_cheek = landmarks[234]\n",
    "        right_cheek = landmarks[454]\n",
    "        left_jaw = landmarks[172]\n",
    "        right_jaw = landmarks[397]\n",
    "        left_eye_left = landmarks[263]      \n",
    "        left_eye_right = landmarks[362]     \n",
    "        right_eye_left = landmarks[33]      \n",
    "        right_eye_right = landmarks[133]    \n",
    "        nose_tip = landmarks[1]\n",
    "        nose_bridge = landmarks[6]\n",
    "        left_eyebrow_inner = landmarks[70]\n",
    "        left_eyebrow_outer = landmarks[107] \n",
    "        right_eyebrow_inner = landmarks[300]\n",
    "        right_eyebrow_outer = landmarks[336] \n",
    "\n",
    "        # 1Ô∏è‚É£ CWJWR - Cheekbone Width to Jaw Width Ratio\n",
    "        cheekbone_width = distance.euclidean(left_cheek, right_cheek)\n",
    "        jaw_width = distance.euclidean(left_jaw, right_jaw)\n",
    "        features['cwjwr'] = cheekbone_width / (jaw_width + 1e-6)\n",
    "\n",
    "        # 2Ô∏è‚É£ CWUFHR - Cheekbone Width to Upper Face Height Ratio\n",
    "        upper_face_height = distance.euclidean(forehead_center, nose_tip)\n",
    "        features['cwufhr'] = cheekbone_width / (upper_face_height + 1e-6)\n",
    "\n",
    "        # 3Ô∏è‚É£ PAR - Perimeter to Area Ratio\n",
    "        face_contour_points = [left_jaw, left_cheek, forehead_center, right_cheek, right_jaw, chin]\n",
    "        perimeter = sum(\n",
    "            distance.euclidean(face_contour_points[i], face_contour_points[(i+1) % len(face_contour_points)])\n",
    "            for i in range(len(face_contour_points))\n",
    "        )\n",
    "        area = 0.5 * abs(sum(\n",
    "            face_contour_points[i][0] * face_contour_points[(i+1) % len(face_contour_points)][1] -\n",
    "            face_contour_points[(i+1) % len(face_contour_points)][0] * face_contour_points[i][1]\n",
    "            for i in range(len(face_contour_points))\n",
    "        ))\n",
    "        features['par'] = perimeter / (area + 1e-6)\n",
    "\n",
    "        # 4Ô∏è‚É£ ASoE - Average Size of Eyes\n",
    "        left_eye_width = distance.euclidean(left_eye_left, left_eye_right)\n",
    "        right_eye_width = distance.euclidean(right_eye_left, right_eye_right)\n",
    "        features['asoe'] = (left_eye_width + right_eye_width) / 2\n",
    "\n",
    "        # 5Ô∏è‚É£ FHLFHR - Face Height to Lower Face Height Ratio\n",
    "        face_height = distance.euclidean(forehead_center, chin)\n",
    "        lower_face_height = distance.euclidean(nose_tip, chin)\n",
    "        features['fhlfhr'] = face_height / (lower_face_height + 1e-6)\n",
    "\n",
    "        # 6Ô∏è‚É£ FWLFHR - Face Width to Lower Face Height Ratio\n",
    "        face_width = distance.euclidean(left_jaw, right_jaw)\n",
    "        features['fwlfhr'] = face_width / (lower_face_height + 1e-6)\n",
    "\n",
    "        # 7Ô∏è‚É£ MEH - Mean Eyebrow Height\n",
    "        left_eyebrow_height = distance.euclidean(\n",
    "            (left_eyebrow_inner + left_eyebrow_outer) / 2, left_eye_left\n",
    "        )\n",
    "        right_eyebrow_height = distance.euclidean(\n",
    "            (right_eyebrow_inner + right_eyebrow_outer) / 2, right_eye_left\n",
    "        )\n",
    "        features['meh'] = (left_eyebrow_height + right_eyebrow_height) / 2\n",
    "\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error calculating features: {e}\")\n",
    "        return None\n",
    "\n",
    "# ==== Processing ====\n",
    "data = []\n",
    "processed_images = []\n",
    "\n",
    "for path in tqdm(image_paths, desc=\"Processing images\", ncols=80):\n",
    "    img = cv2.imread(path)\n",
    "    if img is None:\n",
    "        continue\n",
    "\n",
    "    h, w, _ = img.shape\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(img_rgb)\n",
    "\n",
    "    if not results.multi_face_landmarks:\n",
    "        continue\n",
    "\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "        coords = np.array([[lm.x * w, lm.y * h] for lm in face_landmarks.landmark])\n",
    "        feats = calculate_geometric_features(coords)\n",
    "        if feats:\n",
    "            feats['filename'] = os.path.basename(path)\n",
    "            data.append(feats)\n",
    "\n",
    "        # Draw landmarks\n",
    "        for idx, (x, y) in enumerate(coords.astype(int)):\n",
    "            cv2.circle(img, (x, y), 1, (0,255, 0), -1)\n",
    "            if idx % 25 == 0:  # fewer labels for readability\n",
    "                cv2.putText(img, str(idx), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0,0,255), 1)\n",
    "\n",
    "    save_path = os.path.join(output_folder, os.path.basename(path))\n",
    "    cv2.imwrite(save_path, img)\n",
    "\n",
    "    if len(processed_images) < 10:\n",
    "        processed_images.append(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# ==== Save CSV ====\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"\\n‚úÖ Saved {len(df)} entries to {csv_path}\")\n",
    "\n",
    "# ==== Show sample outputs ====\n",
    "if processed_images:\n",
    "    plt.figure(figsize=(30, 20))\n",
    "    for i, img in enumerate(processed_images):\n",
    "        plt.subplot(2, 5, i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Image {i+1}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No landmarks detected in any image.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-27T03:18:06.956344Z",
     "iopub.status.idle": "2025-10-27T03:18:06.956917Z",
     "shell.execute_reply": "2025-10-27T03:18:06.956758Z",
     "shell.execute_reply.started": "2025-10-27T03:18:06.956742Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ==== 1Ô∏è‚É£ Paths ====\n",
    "person_csv = \"/kaggle/input/person.cs\"          # adjust if it's in a subfolder\n",
    "features_csv = \"/kaggle/working/features.csv\"    # output file from your previous step\n",
    "output_csv = \"/kaggle/working/merged_features.csv\"\n",
    "\n",
    "# ==== 2Ô∏è‚É£ Load the CSV files ====\n",
    "df_person = pd.read_csv(person_csv)\n",
    "df_features = pd.read_csv(features_csv)\n",
    "\n",
    "print(\"‚úÖ Loaded files:\")\n",
    "print(f\"person.csv ‚Üí {df_person.shape}\")\n",
    "print(f\"features.csv ‚Üí {df_features.shape}\")\n",
    "\n",
    "# ==== 3Ô∏è‚É£ Check column names ====\n",
    "print(\"\\nüß© Columns in person.csv:\", df_person.columns.tolist())\n",
    "print(\"üß© Columns in features.csv:\", df_features.columns.tolist())\n",
    "\n",
    "# ==== 4Ô∏è‚É£ Merge using 'ID' (change to correct column if needed) ====\n",
    "# Common column name can be 'ID', 'id', or 'person_id' ‚Äî adjust if necessary\n",
    "merge_key = \"id\"\n",
    "if merge_key not in df_person.columns or merge_key not in df_features.columns:\n",
    "    # Try to auto-detect possible matching column\n",
    "    possible_keys = set(df_person.columns) & set(df_features.columns)\n",
    "    if len(possible_keys) > 0:\n",
    "        merge_key = list(possible_keys)[0]\n",
    "        print(f\"‚öôÔ∏è Auto-detected merge key: {merge_key}\")\n",
    "    else:\n",
    "        raise ValueError(\"‚ùå No common column found for merging!\")\n",
    "\n",
    "merged_df = pd.merge(df_person, df_features, on=merge_key, how=\"inner\")\n",
    "\n",
    "\n",
    "merged_df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Merged successfully! Saved to: {output_csv}\")\n",
    "print(\"üßæ Final shape:\", merged_df.shape)\n",
    "\n",
    "\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üß† Part 2: Model Training, Validation & Ablation Study\n",
    "### Complete Training Pipeline with Multiple Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-11T16:40:51.691089Z",
     "iopub.status.busy": "2026-01-11T16:40:51.690451Z",
     "iopub.status.idle": "2026-01-11T16:40:51.715571Z",
     "shell.execute_reply": "2026-01-11T16:40:51.715028Z",
     "shell.execute_reply.started": "2026-01-11T16:40:51.691061Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import warnings\n",
    "import os\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# GPU & SEED SETUP\n",
    "# ============================================================================\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.enabled = True\n",
    "        if torch.cuda.get_device_capability()[0] >= 8:\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        return device\n",
    "    print(\"Using CPU\")\n",
    "    return torch.device('cpu')\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ============================================================================\n",
    "# GNN SETUP (Optional)\n",
    "# ============================================================================\n",
    "\n",
    "try:\n",
    "    from torch_geometric.nn import GCNConv, global_mean_pool, global_max_pool, BatchNorm\n",
    "    GNN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GNN_AVAILABLE = False\n",
    "    try:\n",
    "        os.system('pip install torch-geometric -q')\n",
    "        from torch_geometric.nn import GCNConv, global_mean_pool, global_max_pool, BatchNorm\n",
    "        GNN_AVAILABLE = True\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "DATA_DIR = '/kaggle/input/bmi-dataset'\n",
    "OUTPUT_DIR = '/kaggle/working'\n",
    "TRAIN_CSV = f'{DATA_DIR}/train.csv'\n",
    "TEST_CSV = f'{DATA_DIR}/test.csv'\n",
    "IMAGE_DIR = f'{DATA_DIR}/ROI'\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# DATA LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_raw_data():\n",
    "    \"\"\"Load data WITHOUT any NaN filling - to prevent data leakage\"\"\"\n",
    "    train_df = pd.read_csv(TRAIN_CSV)\n",
    "    test_df = pd.read_csv(TEST_CSV)\n",
    "    print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")\n",
    "    return train_df, test_df\n",
    "\n",
    "def fill_nan_from_reference(df, reference_df):\n",
    "    \"\"\"Fill NaN values in df using statistics from reference_df (training data only)\"\"\"\n",
    "    df = df.copy()\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        if df[col].isna().any():\n",
    "            fill_value = reference_df[col].median() if col in reference_df.columns else 0\n",
    "            df[col] = df[col].fillna(fill_value)\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        if df[col].isna().any():\n",
    "            if col in reference_df.columns and not reference_df[col].mode().empty:\n",
    "                df[col] = df[col].fillna(reference_df[col].mode()[0])\n",
    "    return df\n",
    "\n",
    "def get_landmark_info(df):\n",
    "    \"\"\"Extract landmark column information\"\"\"\n",
    "    x_cols = sorted([c for c in df.columns if c.endswith('_x')])\n",
    "    landmarks = []\n",
    "    for x_col in x_cols:\n",
    "        prefix = x_col[:-2]\n",
    "        y_col = f'{prefix}_y'\n",
    "        z_col = f'{prefix}_z'\n",
    "        if y_col in df.columns:\n",
    "            landmarks.append((prefix, x_col, y_col, z_col if z_col in df.columns else None))\n",
    "    return landmarks\n",
    "\n",
    "def get_base_feature_cols(df):\n",
    "    \"\"\"Get list of base features that exist in the dataframe\"\"\"\n",
    "    basic = ['face_height', 'face_width_cheeks', 'face_width_jaw', 'face_ratio_height_width',\n",
    "             'jaw_cheek_ratio', 'left_eye_width', 'right_eye_width', 'eye_width_ratio',\n",
    "             'nose_length', 'nose_width', 'nose_ratio', 'mouth_width', 'face_oval_area',\n",
    "             'face_oval_perimeter', 'face_compactness', 'age', 'sex_encoded',\n",
    "             'race_Asian', 'race_Black', 'race_Hispanic', 'race_White']\n",
    "    return [f for f in basic if f in df.columns]\n",
    "\n",
    "def engineer_features_for_fold(train_df, val_df=None, test_df=None):\n",
    "    \"\"\"Engineer features for a fold without data leakage\"\"\"\n",
    "    train_df = fill_nan_from_reference(train_df, train_df)\n",
    "    if val_df is not None:\n",
    "        val_df = fill_nan_from_reference(val_df, train_df)\n",
    "    if test_df is not None:\n",
    "        test_df = fill_nan_from_reference(test_df, train_df)\n",
    "\n",
    "    dfs = {'train': train_df.copy()}\n",
    "    if val_df is not None:\n",
    "        dfs['val'] = val_df.copy()\n",
    "    if test_df is not None:\n",
    "        dfs['test'] = test_df.copy()\n",
    "\n",
    "    features = get_base_feature_cols(train_df)\n",
    "\n",
    "    for name, df in dfs.items():\n",
    "        if 'face_height' in df.columns and 'face_width_cheeks' in df.columns:\n",
    "            df['fwhr'] = df['face_width_cheeks'] / df['face_height'].clip(lower=1e-8)\n",
    "            df['face_area'] = df['face_width_cheeks'] * df['face_height']\n",
    "\n",
    "        if 'left_eye_width' in df.columns and 'right_eye_width' in df.columns:\n",
    "            sum_eyes = df['left_eye_width'] + df['right_eye_width']\n",
    "            df['eye_symmetry'] = 1 - abs(df['left_eye_width'] - df['right_eye_width']) / sum_eyes.clip(lower=1e-8)\n",
    "\n",
    "        if 'nose_length' in df.columns and 'nose_width' in df.columns:\n",
    "            df['nose_compactness'] = df['nose_width'] / df['nose_length'].clip(lower=1e-8)\n",
    "\n",
    "        if 'age' in df.columns:\n",
    "            df['age_squared'] = df['age'] ** 2\n",
    "            df['age_log'] = np.log1p(df['age'])\n",
    "\n",
    "        dfs[name] = df\n",
    "\n",
    "    derived = ['fwhr', 'face_area', 'eye_symmetry', 'nose_compactness', 'age_squared', 'age_log']\n",
    "    features.extend([f for f in derived if f in dfs['train'].columns])\n",
    "\n",
    "    for name, df in dfs.items():\n",
    "        for suffix, coord_name in [('_x', 'x'), ('_y', 'y'), ('_z', 'z')]:\n",
    "            cols = [c for c in df.columns if c.endswith(suffix)]\n",
    "            if cols:\n",
    "                df[f'coord_mean_{coord_name}'] = df[cols].mean(axis=1)\n",
    "                df[f'coord_std_{coord_name}'] = df[cols].std(axis=1)\n",
    "                df[f'coord_range_{coord_name}'] = df[cols].max(axis=1) - df[cols].min(axis=1)\n",
    "        dfs[name] = df\n",
    "\n",
    "    for coord_name in ['x', 'y', 'z']:\n",
    "        for stat in ['mean', 'std', 'range']:\n",
    "            feat_name = f'coord_{stat}_{coord_name}'\n",
    "            if feat_name in dfs['train'].columns:\n",
    "                features.append(feat_name)\n",
    "\n",
    "    features = list(set(features))\n",
    "\n",
    "    for name, df in dfs.items():\n",
    "        for f in features:\n",
    "            if f in df.columns:\n",
    "                df[f] = df[f].fillna(0)\n",
    "        dfs[name] = df\n",
    "\n",
    "    results = [dfs['train']]\n",
    "    if val_df is not None:\n",
    "        results.append(dfs['val'])\n",
    "    if test_df is not None:\n",
    "        results.append(dfs['test'])\n",
    "    results.append(features)\n",
    "\n",
    "    return tuple(results)\n",
    "\n",
    "def create_graphs(num_nodes):\n",
    "    \"\"\"Create multi-scale graph structures\"\"\"\n",
    "    if num_nodes <= 0:\n",
    "        return None\n",
    "\n",
    "    graphs = {}\n",
    "\n",
    "    if num_nodes <= 10:\n",
    "        edges = [[i, j] for i in range(num_nodes) for j in range(num_nodes) if i != j]\n",
    "        edge_tensor = torch.tensor(edges, dtype=torch.long).t().contiguous() if edges else torch.zeros(2, 0, dtype=torch.long)\n",
    "        graphs['local'] = graphs['regional'] = graphs['global'] = edge_tensor\n",
    "        return graphs\n",
    "\n",
    "    local_edges = []\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(1, min(4, num_nodes)):\n",
    "            if i + j < num_nodes:\n",
    "                local_edges.extend([[i, i + j], [i + j, i]])\n",
    "\n",
    "    regional_edges = []\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(1, min(7, num_nodes)):\n",
    "            if i + j < num_nodes:\n",
    "                regional_edges.extend([[i, i + j], [i + j, i]])\n",
    "    for i in range(0, num_nodes - 5, 5):\n",
    "        for j in range(i + 5, min(i + 15, num_nodes), 5):\n",
    "            regional_edges.extend([[i, j], [j, i]])\n",
    "\n",
    "    global_edges = []\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(1, min(13, num_nodes)):\n",
    "            if i + j < num_nodes:\n",
    "                global_edges.extend([[i, i + j], [i + j, i]])\n",
    "    mid = num_nodes // 2\n",
    "    for i in range(min(mid, num_nodes - mid)):\n",
    "        if mid + i < num_nodes:\n",
    "            global_edges.extend([[i, mid + i], [mid + i, i]])\n",
    "    step = max(num_nodes // 8, 1)\n",
    "    for i in range(0, num_nodes, step):\n",
    "        for j in range(i + step, num_nodes, step):\n",
    "            global_edges.extend([[i, j], [j, i]])\n",
    "\n",
    "    graphs['local'] = torch.tensor(local_edges, dtype=torch.long).t().contiguous()\n",
    "    graphs['regional'] = torch.tensor(regional_edges, dtype=torch.long).t().contiguous()\n",
    "    graphs['global'] = torch.tensor(global_edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    return graphs\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET\n",
    "# ============================================================================\n",
    "\n",
    "class BMIDataset(Dataset):\n",
    "    def __init__(self, df, feature_cols, landmark_info, image_dir,\n",
    "                 is_training=True, augment_mult=1,\n",
    "                 feature_scaler=None, landmark_scaler=None,\n",
    "                 fit_scalers=False):\n",
    "\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.feature_cols = feature_cols\n",
    "        self.landmark_info = landmark_info\n",
    "        self.image_dir = image_dir\n",
    "        self.is_training = is_training\n",
    "        self.augment_mult = augment_mult if is_training else 1\n",
    "\n",
    "        raw_features = np.nan_to_num(self.df[feature_cols].values.astype(np.float32))\n",
    "\n",
    "        if fit_scalers or feature_scaler is None:\n",
    "            self.feature_scaler = RobustScaler()\n",
    "            scaled_features = self.feature_scaler.fit_transform(raw_features)\n",
    "        else:\n",
    "            self.feature_scaler = feature_scaler\n",
    "            scaled_features = self.feature_scaler.transform(raw_features)\n",
    "\n",
    "        self.features = torch.from_numpy(scaled_features.astype(np.float32))\n",
    "\n",
    "        self.targets = torch.from_numpy(\n",
    "            np.nan_to_num(self.df['BMI'].values.astype(np.float32), nan=self.df['BMI'].median())\n",
    "        )\n",
    "\n",
    "        if 'age' in self.df.columns:\n",
    "            self.age_targets = torch.from_numpy(\n",
    "                np.nan_to_num(self.df['age'].values.astype(np.float32), nan=self.df['age'].median())\n",
    "            )\n",
    "        else:\n",
    "            self.age_targets = torch.zeros(len(self.df))\n",
    "\n",
    "        if 'sex_encoded' in self.df.columns:\n",
    "            sex = np.clip(np.nan_to_num(self.df['sex_encoded'].values, nan=0).astype(np.int64), 0, 1)\n",
    "            self.sex_targets = torch.from_numpy(sex)\n",
    "        else:\n",
    "            self.sex_targets = torch.zeros(len(self.df), dtype=torch.long)\n",
    "\n",
    "        bmi_vals = np.nan_to_num(self.df['BMI'].values, nan=self.df['BMI'].median())\n",
    "        bins = [0, 18.5, 25, 30, 100]\n",
    "        cat = pd.cut(bmi_vals, bins=bins, labels=[0, 1, 2, 3], include_lowest=True)\n",
    "        cat = pd.Series(cat).cat.codes.fillna(1).astype(np.int64).clip(0, 3)\n",
    "        self.bmi_categories = torch.from_numpy(cat.values)\n",
    "\n",
    "        self.filenames = self.df['image_filename'].tolist()\n",
    "\n",
    "        self.landmarks, self.landmark_scaler = self._extract_landmarks(\n",
    "            landmark_scaler, fit_scalers\n",
    "        )\n",
    "\n",
    "        if is_training:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(224, scale=(0.75, 1.0), ratio=(0.9, 1.1)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.2, hue=0.08),\n",
    "                transforms.RandomRotation(degrees=12),\n",
    "                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "                transforms.RandomErasing(p=0.1, scale=(0.02, 0.1)),\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "\n",
    "        self._placeholder = None\n",
    "\n",
    "    def _extract_landmarks(self, scaler, fit_scaler):\n",
    "        coord_cols = [c for c in self.df.columns if c.endswith(('_x', '_y', '_z'))]\n",
    "        if not coord_cols:\n",
    "            return None, None\n",
    "\n",
    "        prefixes = sorted(set(c.rsplit('_', 1)[0] for c in coord_cols))\n",
    "        has_z = any(c.endswith('_z') for c in coord_cols)\n",
    "        dim = 3 if has_z else 2\n",
    "\n",
    "        data = np.zeros((len(self.df), len(prefixes), dim), dtype=np.float32)\n",
    "        for i, row in self.df.iterrows():\n",
    "            for j, prefix in enumerate(prefixes):\n",
    "                data[i, j, 0] = row.get(f'{prefix}_x', 0) or 0\n",
    "                data[i, j, 1] = row.get(f'{prefix}_y', 0) or 0\n",
    "                if has_z:\n",
    "                    data[i, j, 2] = row.get(f'{prefix}_z', 0) or 0\n",
    "\n",
    "        data = np.nan_to_num(data)\n",
    "        flat = data.reshape(-1, dim)\n",
    "\n",
    "        if fit_scaler or scaler is None:\n",
    "            fitted_scaler = StandardScaler()\n",
    "            flat_scaled = fitted_scaler.fit_transform(flat)\n",
    "        else:\n",
    "            fitted_scaler = scaler\n",
    "            flat_scaled = fitted_scaler.transform(flat)\n",
    "\n",
    "        return torch.from_numpy(flat_scaled.reshape(data.shape).astype(np.float32)), fitted_scaler\n",
    "\n",
    "    def _get_placeholder(self):\n",
    "        if self._placeholder is None:\n",
    "            self._placeholder = torch.zeros(3, 224, 224)\n",
    "            self._placeholder[0] = 0.485\n",
    "            self._placeholder[1] = 0.456\n",
    "            self._placeholder[2] = 0.406\n",
    "        return self._placeholder.clone()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) * self.augment_mult\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        orig_idx = idx % len(self.df)\n",
    "        is_aug = idx >= len(self.df)\n",
    "\n",
    "        feat = self.features[orig_idx].clone()\n",
    "        if self.is_training and is_aug:\n",
    "            noise_std = 0.02\n",
    "            feat = feat + torch.randn_like(feat) * noise_std\n",
    "\n",
    "        if self.landmarks is not None:\n",
    "            graph_feat = self.landmarks[orig_idx].clone()\n",
    "            if self.is_training and is_aug:\n",
    "                graph_feat = graph_feat + torch.randn_like(graph_feat) * 0.01\n",
    "        else:\n",
    "            graph_feat = torch.zeros(1, 3)\n",
    "\n",
    "        img_path = os.path.join(self.image_dir, self.filenames[orig_idx])\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                if img.mode != 'RGB':\n",
    "                    img = img.convert('RGB')\n",
    "                image = self.transform(img)\n",
    "        except Exception as e:\n",
    "            image = self._get_placeholder()\n",
    "\n",
    "        return {\n",
    "            'features': feat,\n",
    "            'image': image,\n",
    "            'graph_features': graph_feat,\n",
    "            'target': self.targets[orig_idx],\n",
    "            'age_target': self.age_targets[orig_idx],\n",
    "            'sex_target': self.sex_targets[orig_idx],\n",
    "            'bmi_category': self.bmi_categories[orig_idx],\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'features': torch.stack([b['features'] for b in batch]),\n",
    "        'image': torch.stack([b['image'] for b in batch]),\n",
    "        'graph_features': torch.stack([b['graph_features'] for b in batch]),\n",
    "        'target': torch.stack([b['target'] for b in batch]),\n",
    "        'age_target': torch.stack([b['age_target'] for b in batch]),\n",
    "        'sex_target': torch.stack([b['sex_target'] for b in batch]),\n",
    "        'bmi_category': torch.stack([b['bmi_category'] for b in batch]),\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL COMPONENTS\n",
    "# ============================================================================\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, dim, reduction=4):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim // reduction, dim, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, batch=None):\n",
    "        if batch is None:\n",
    "            avg = x.mean(0, keepdim=True)\n",
    "            mx = x.max(0, keepdim=True)[0]\n",
    "            return x * self.sigmoid(self.mlp(avg) + self.mlp(mx))\n",
    "\n",
    "        bs = batch.max().item() + 1\n",
    "        avgs, mxs = [], []\n",
    "        for i in range(bs):\n",
    "            mask = batch == i\n",
    "            if mask.any():\n",
    "                avgs.append(x[mask].mean(0))\n",
    "                mxs.append(x[mask].max(0)[0])\n",
    "        if not avgs:\n",
    "            return x\n",
    "        avg = torch.stack(avgs)\n",
    "        mx = torch.stack(mxs)\n",
    "        return x * self.sigmoid(self.mlp(avg) + self.mlp(mx))[batch]\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Linear(2, dim // 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim // 4, 1)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, batch=None):\n",
    "        avg = x.mean(-1, keepdim=True)\n",
    "        mx = x.max(-1, keepdim=True)[0]\n",
    "        att = self.sigmoid(self.conv(torch.cat([avg, mx], -1)))\n",
    "        return x * att\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, dim, reduction=4):\n",
    "        super().__init__()\n",
    "        self.channel = ChannelAttention(dim, reduction)\n",
    "        self.spatial = SpatialAttention(dim)\n",
    "\n",
    "    def forward(self, x, batch=None):\n",
    "        x = self.channel(x, batch)\n",
    "        x = self.spatial(x, batch)\n",
    "        return x\n",
    "\n",
    "if GNN_AVAILABLE:\n",
    "    class MultiScaleGCN(nn.Module):\n",
    "        def __init__(self, in_dim=3, hidden=128, out_dim=256, layers=3, dropout=0.3):\n",
    "            super().__init__()\n",
    "            self.input_proj = nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden), BatchNorm(hidden), nn.ReLU(), nn.Dropout(dropout)\n",
    "            )\n",
    "\n",
    "            self.local_convs = nn.ModuleList([GCNConv(hidden, hidden, improved=True) for _ in range(layers)])\n",
    "            self.regional_convs = nn.ModuleList([GCNConv(hidden, hidden, improved=True) for _ in range(layers)])\n",
    "            self.global_convs = nn.ModuleList([GCNConv(hidden, hidden, improved=True) for _ in range(layers)])\n",
    "            self.bns = nn.ModuleList([BatchNorm(hidden) for _ in range(layers)])\n",
    "            self.cbams = nn.ModuleList([CBAM(hidden) for _ in range(layers)])\n",
    "\n",
    "            self.cross_attn = nn.MultiheadAttention(hidden, 4, dropout=dropout, batch_first=True)\n",
    "            self.norm = nn.LayerNorm(hidden)\n",
    "            self.node_attn = nn.Sequential(nn.Linear(hidden, hidden // 2), nn.ReLU(), nn.Linear(hidden // 2, 1))\n",
    "\n",
    "            self.output = nn.Sequential(\n",
    "                nn.Linear(hidden * 3, out_dim), nn.BatchNorm1d(out_dim), nn.ReLU(), nn.Dropout(dropout),\n",
    "                nn.Linear(out_dim, out_dim), nn.BatchNorm1d(out_dim), nn.ReLU()\n",
    "            )\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x, edges, batch):\n",
    "            bs = batch.max().item() + 1\n",
    "            x = self.input_proj(x)\n",
    "            x_l, x_r, x_g = x.clone(), x.clone(), x.clone()\n",
    "\n",
    "            for lc, rc, gc, bn, cbam in zip(self.local_convs, self.regional_convs, self.global_convs, self.bns, self.cbams):\n",
    "                x_l = self.dropout(cbam(torch.relu(bn(lc(x_l, edges['local']))), batch)) + x_l\n",
    "                x_r = self.dropout(cbam(torch.relu(bn(rc(x_r, edges['regional']))), batch)) + x_r\n",
    "                x_g = self.dropout(cbam(torch.relu(bn(gc(x_g, edges['global']))), batch)) + x_g\n",
    "\n",
    "            fused = []\n",
    "            for i in range(bs):\n",
    "                mask = batch == i\n",
    "                if mask.any():\n",
    "                    scales = torch.stack([x_l[mask], x_r[mask], x_g[mask]], 1)\n",
    "                    attn_out, _ = self.cross_attn(scales, scales, scales)\n",
    "                    fused.append(self.norm(attn_out + scales).mean(1))\n",
    "\n",
    "            if not fused:\n",
    "                return torch.zeros(bs, self.output[0].out_features, device=x.device)\n",
    "\n",
    "            x_fused = torch.cat(fused, 0)\n",
    "            x_mean = global_mean_pool(x_fused, batch)\n",
    "            x_max = global_max_pool(x_fused, batch)\n",
    "            attn_w = torch.softmax(self.node_attn(x_fused), 0)\n",
    "            x_attn = global_mean_pool(x_fused * attn_w, batch)\n",
    "\n",
    "            return self.output(torch.cat([x_mean, x_max, x_attn], 1))\n",
    "else:\n",
    "    class MultiScaleGCN(nn.Module):\n",
    "        def __init__(self, in_dim=3, hidden=128, out_dim=256, layers=3, dropout=0.3):\n",
    "            super().__init__()\n",
    "            self.input_proj = nn.Linear(in_dim, hidden)\n",
    "            self.attns = nn.ModuleList([\n",
    "                nn.MultiheadAttention(hidden, 4, dropout=dropout, batch_first=True) for _ in range(layers)\n",
    "            ])\n",
    "            self.norms = nn.ModuleList([nn.LayerNorm(hidden) for _ in range(layers)])\n",
    "            self.output = nn.Sequential(\n",
    "                nn.Linear(hidden, out_dim), nn.BatchNorm1d(out_dim), nn.ReLU(), nn.Dropout(dropout),\n",
    "                nn.Linear(out_dim, out_dim), nn.BatchNorm1d(out_dim), nn.ReLU()\n",
    "            )\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x, edges=None, batch=None):\n",
    "            if x.dim() == 2:\n",
    "                x = x.unsqueeze(0)\n",
    "            x = torch.relu(self.input_proj(x))\n",
    "            for attn, norm in zip(self.attns, self.norms):\n",
    "                res = x\n",
    "                x, _ = attn(x, x, x)\n",
    "                x = norm(self.dropout(x) + res)\n",
    "            return self.output(x.mean(1))\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN MODEL\n",
    "# ============================================================================\n",
    "\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, num_features, num_landmarks=0, landmark_dim=3, dropout=0.3, use_gcn=True):\n",
    "        super().__init__()\n",
    "        self.use_gcn = use_gcn and num_landmarks > 0\n",
    "\n",
    "        resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        for p in list(resnet.parameters())[:-20]:\n",
    "            p.requires_grad = False\n",
    "        self.img_backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.img_proj = nn.Sequential(\n",
    "            nn.Linear(512, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.tab_net = nn.Sequential(\n",
    "            nn.Linear(num_features, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(dropout * 0.7),\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(dropout * 0.5)\n",
    "        )\n",
    "\n",
    "        gcn_dim = 256 if self.use_gcn else 0\n",
    "        if self.use_gcn:\n",
    "            self.gcn = MultiScaleGCN(landmark_dim, 128, gcn_dim, 3, dropout)\n",
    "\n",
    "        combined = 512 + 128 + gcn_dim\n",
    "        self.fusion_attn = nn.MultiheadAttention(combined, 8, dropout=dropout, batch_first=True)\n",
    "        self.fusion_norm = nn.LayerNorm(combined)\n",
    "\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(combined, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(dropout * 0.7)\n",
    "        )\n",
    "\n",
    "        self.bmi_head = nn.Sequential(\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(dropout * 0.5),\n",
    "            nn.Linear(128, 64), nn.BatchNorm1d(64), nn.ReLU(), nn.Linear(64, 1)\n",
    "        )\n",
    "        self.age_head = nn.Sequential(\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(dropout * 0.5), nn.Linear(128, 1)\n",
    "        )\n",
    "        self.sex_head = nn.Sequential(\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(dropout * 0.5), nn.Linear(128, 2)\n",
    "        )\n",
    "        self.cat_head = nn.Sequential(\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(dropout * 0.5), nn.Linear(128, 4)\n",
    "        )\n",
    "\n",
    "        self.log_vars = nn.Parameter(torch.zeros(4))\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, (nn.BatchNorm1d, nn.LayerNorm)):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, features, image, graph_features=None, edges=None):\n",
    "        bs = features.size(0)\n",
    "\n",
    "        img_feat = self.img_backbone(image).view(bs, -1)\n",
    "        img_feat = self.img_proj(img_feat)\n",
    "\n",
    "        tab_feat = self.tab_net(features)\n",
    "\n",
    "        if self.use_gcn and graph_features is not None:\n",
    "            if GNN_AVAILABLE and edges is not None:\n",
    "                num_nodes = graph_features.size(1)\n",
    "                x = graph_features.view(-1, graph_features.size(-1))\n",
    "                batch_idx = torch.arange(bs, device=x.device).repeat_interleave(num_nodes)\n",
    "\n",
    "                edges_batch = {}\n",
    "                for scale in ['local', 'regional', 'global']:\n",
    "                    edges_batch[scale] = torch.cat([\n",
    "                        edges[scale] + i * num_nodes for i in range(bs)\n",
    "                    ], dim=1).to(x.device)\n",
    "\n",
    "                gcn_feat = self.gcn(x, edges_batch, batch_idx)\n",
    "            else:\n",
    "                gcn_feat = self.gcn(graph_features)\n",
    "            combined = torch.cat([img_feat, tab_feat, gcn_feat], 1)\n",
    "        else:\n",
    "            combined = torch.cat([img_feat, tab_feat], 1)\n",
    "\n",
    "        combined_u = combined.unsqueeze(1)\n",
    "        attn_out, _ = self.fusion_attn(combined_u, combined_u, combined_u)\n",
    "        fused = self.fusion_norm(attn_out.squeeze(1) + combined)\n",
    "\n",
    "        shared = self.shared(fused)\n",
    "\n",
    "        return {\n",
    "            'bmi': self.bmi_head(shared).squeeze(-1),\n",
    "            'age': self.age_head(shared).squeeze(-1),\n",
    "            'sex': self.sex_head(shared),\n",
    "            'bmi_category': self.cat_head(shared)\n",
    "        }\n",
    "\n",
    "# ============================================================================\n",
    "# LOSS & TRAINING UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "class MultiTaskLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.smooth_l1 = nn.SmoothL1Loss()\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, preds, targets, log_vars):\n",
    "        bmi_loss = self.smooth_l1(preds['bmi'], targets['bmi'])\n",
    "        age_loss = self.smooth_l1(preds['age'], targets['age'])\n",
    "        sex_loss = self.ce(preds['sex'], targets['sex'])\n",
    "        cat_loss = self.ce(preds['bmi_category'], targets['bmi_category'])\n",
    "\n",
    "        total = (torch.exp(-log_vars[0]) * bmi_loss + log_vars[0] +\n",
    "                 torch.exp(-log_vars[1]) * age_loss + log_vars[1] +\n",
    "                 torch.exp(-log_vars[2]) * sex_loss + log_vars[2] +\n",
    "                 torch.exp(-log_vars[3]) * cat_loss + log_vars[3])\n",
    "\n",
    "        return total, {'bmi': bmi_loss.item(), 'age': age_loss.item(),\n",
    "                       'sex': sex_loss.item(), 'cat': cat_loss.item(), 'total': total.item()}\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0.0001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.best_state = None\n",
    "\n",
    "    def __call__(self, score, model):\n",
    "        if self.best_score is None or score < self.best_score - self.min_delta:\n",
    "            self.best_score = score\n",
    "            self.best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device, edges=None):\n",
    "    model.train()\n",
    "    total_loss, total_bmi = 0, 0\n",
    "    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
    "\n",
    "    for batch in loader:\n",
    "        feat = batch['features'].to(device, non_blocking=True)\n",
    "        img = batch['image'].to(device, non_blocking=True)\n",
    "        graph = batch['graph_features'].to(device, non_blocking=True)\n",
    "        targets = {'bmi': batch['target'].to(device, non_blocking=True),\n",
    "                   'age': batch['age_target'].to(device, non_blocking=True),\n",
    "                   'sex': batch['sex_target'].to(device, non_blocking=True),\n",
    "                   'bmi_category': batch['bmi_category'].to(device, non_blocking=True)}\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if scaler:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                preds = model(feat, img, graph, edges)\n",
    "                loss, losses = criterion(preds, targets, model.log_vars)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            preds = model(feat, img, graph, edges)\n",
    "            loss, losses = criterion(preds, targets, model.log_vars)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += losses['total']\n",
    "        total_bmi += losses['bmi']\n",
    "\n",
    "    return total_loss / len(loader), total_bmi / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_epoch(model, loader, criterion, device, edges=None):\n",
    "    model.eval()\n",
    "    total_loss, total_bmi = 0, 0\n",
    "    all_preds, all_targets = [], []\n",
    "\n",
    "    for batch in loader:\n",
    "        feat = batch['features'].to(device, non_blocking=True)\n",
    "        img = batch['image'].to(device, non_blocking=True)\n",
    "        graph = batch['graph_features'].to(device, non_blocking=True)\n",
    "        targets = {'bmi': batch['target'].to(device, non_blocking=True),\n",
    "                   'age': batch['age_target'].to(device, non_blocking=True),\n",
    "                   'sex': batch['sex_target'].to(device, non_blocking=True),\n",
    "                   'bmi_category': batch['bmi_category'].to(device, non_blocking=True)}\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            with torch.cuda.amp.autocast():\n",
    "                preds = model(feat, img, graph, edges)\n",
    "                loss, losses = criterion(preds, targets, model.log_vars)\n",
    "        else:\n",
    "            preds = model(feat, img, graph, edges)\n",
    "            loss, losses = criterion(preds, targets, model.log_vars)\n",
    "\n",
    "        total_loss += losses['total']\n",
    "        total_bmi += losses['bmi']\n",
    "        all_preds.extend(preds['bmi'].cpu().numpy())\n",
    "        all_targets.extend(targets['bmi'].cpu().numpy())\n",
    "\n",
    "    n = len(loader)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "    mae = mean_absolute_error(all_targets, all_preds)\n",
    "    return total_loss / n, total_bmi / n, r2, mae, all_preds, all_targets\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    device = get_device()\n",
    "\n",
    "    batch_size = 128 if device.type == 'cuda' else 32\n",
    "    num_workers = 4 if device.type == 'cuda' else 2\n",
    "    \n",
    "    num_epochs = 15\n",
    "    patience = 8\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Loading Raw Data\")\n",
    "    print(\"=\"*60)\n",
    "    train_df_raw, test_df_raw = load_raw_data()\n",
    "\n",
    "    landmark_info = get_landmark_info(train_df_raw)\n",
    "    num_landmarks = len(landmark_info)\n",
    "    landmark_dim = 3 if any(l[3] is not None for l in landmark_info) else 2\n",
    "    print(f\"Landmarks: {num_landmarks}, Dim: {landmark_dim}\")\n",
    "\n",
    "    edges = create_graphs(num_landmarks)\n",
    "    if edges:\n",
    "        print(f\"Graph edges - L:{edges['local'].shape[1]} R:{edges['regional'].shape[1]} G:{edges['global'].shape[1]}\")\n",
    "\n",
    "    train_df_raw['bmi_bin'] = pd.qcut(train_df_raw['BMI'], q=5, labels=False, duplicates='drop')\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"5-Fold Cross Validation (No Data Leakage)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    cv_results = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df_raw, train_df_raw['bmi_bin']), 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"FOLD {fold}/5\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        fold_train_raw = train_df_raw.iloc[train_idx].copy()\n",
    "        fold_val_raw = train_df_raw.iloc[val_idx].copy()\n",
    "        print(f\"Train samples: {len(fold_train_raw)}, Validation samples: {len(fold_val_raw)}\")\n",
    "\n",
    "        fold_train, fold_val, feature_cols = engineer_features_for_fold(\n",
    "            fold_train_raw, fold_val_raw\n",
    "        )\n",
    "\n",
    "        print(f\"Number of features: {len(feature_cols)}\")\n",
    "\n",
    "        train_ds = BMIDataset(\n",
    "            fold_train, feature_cols, landmark_info, IMAGE_DIR,\n",
    "            is_training=True, augment_mult=3,\n",
    "            fit_scalers=True\n",
    "        )\n",
    "\n",
    "        val_ds = BMIDataset(\n",
    "            fold_val, feature_cols, landmark_info, IMAGE_DIR,\n",
    "            is_training=False, augment_mult=1,\n",
    "            feature_scaler=train_ds.feature_scaler,\n",
    "            landmark_scaler=train_ds.landmark_scaler,\n",
    "            fit_scalers=False\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_ds, batch_size=batch_size, shuffle=True,\n",
    "            num_workers=num_workers, pin_memory=device.type=='cuda',\n",
    "            collate_fn=collate_fn, persistent_workers=False\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_ds, batch_size=batch_size, shuffle=False,\n",
    "            num_workers=num_workers, pin_memory=device.type=='cuda',\n",
    "            collate_fn=collate_fn, persistent_workers=False\n",
    "        )\n",
    "\n",
    "        model = HybridModel(len(feature_cols), num_landmarks, landmark_dim, 0.3, True).to(device)\n",
    "        edges_device = {k: v.to(device) for k, v in edges.items()} if edges else None\n",
    "\n",
    "        optimizer = optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()),\n",
    "            lr=2e-3, weight_decay=1e-4\n",
    "        )\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=5, T_mult=2, eta_min=1e-6\n",
    "        )\n",
    "        criterion = MultiTaskLoss()\n",
    "        early_stop = EarlyStopping(patience=patience)\n",
    "\n",
    "        best_epoch = 0\n",
    "        print(f\"\\nStarting training for {num_epochs} epochs...\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss, train_bmi = train_epoch(model, train_loader, criterion, optimizer, device, edges_device)\n",
    "            val_loss, val_bmi, val_r2, val_mae, _, _ = validate_epoch(model, val_loader, criterion, device, edges_device)\n",
    "            scheduler.step()\n",
    "\n",
    "            is_best = \"\"\n",
    "            if early_stop.best_score is None or val_bmi < early_stop.best_score - early_stop.min_delta:\n",
    "                best_epoch = epoch + 1\n",
    "                is_best = \" *\"\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}, BMI Loss: {train_bmi:.4f}\")\n",
    "            print(f\"Validation Loss: {val_loss:.4f}, BMI Loss: {val_bmi:.4f}, R2: {val_r2:.4f}, MAE: {val_mae:.2f}{is_best}\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "            early_stop(val_bmi, model)\n",
    "            if early_stop.early_stop:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        model.load_state_dict(early_stop.best_state)\n",
    "        _, _, val_r2, val_mae, preds, targets = validate_epoch(model, val_loader, criterion, device, edges_device)\n",
    "        val_rmse = np.sqrt(mean_squared_error(targets, preds))\n",
    "\n",
    "        print(f\"\\nFold {fold} Best Results (Epoch {best_epoch}):\")\n",
    "        print(f\"  R2: {val_r2:.4f}\")\n",
    "        print(f\"  MAE: {val_mae:.2f}\")\n",
    "        print(f\"  RMSE: {val_rmse:.2f}\")\n",
    "\n",
    "        cv_results.append({\n",
    "            'fold': fold, 'r2': val_r2, 'mae': val_mae, 'rmse': val_rmse,\n",
    "            'best_epoch': best_epoch, 'state': early_stop.best_state,\n",
    "            'feature_scaler': train_ds.feature_scaler,\n",
    "            'landmark_scaler': train_ds.landmark_scaler,\n",
    "            'feature_cols': feature_cols\n",
    "        })\n",
    "\n",
    "        del model, train_ds, val_ds, train_loader, val_loader\n",
    "        clear_memory()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CROSS-VALIDATION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    avg_r2 = np.mean([r['r2'] for r in cv_results])\n",
    "    avg_mae = np.mean([r['mae'] for r in cv_results])\n",
    "    avg_rmse = np.mean([r['rmse'] for r in cv_results])\n",
    "    std_r2 = np.std([r['r2'] for r in cv_results])\n",
    "    std_mae = np.std([r['mae'] for r in cv_results])\n",
    "    std_rmse = np.std([r['rmse'] for r in cv_results])\n",
    "\n",
    "    print(f\"\\nMetrics across all folds:\")\n",
    "    print(f\"  R2:   {avg_r2:.4f} ¬± {std_r2:.4f}\")\n",
    "    print(f\"  MAE:  {avg_mae:.2f} ¬± {std_mae:.2f}\")\n",
    "    print(f\"  RMSE: {avg_rmse:.2f} ¬± {std_rmse:.2f}\")\n",
    "\n",
    "    print(f\"\\nPer-fold results:\")\n",
    "    for r in cv_results:\n",
    "        print(f\"  Fold {r['fold']}: R2={r['r2']:.4f}, MAE={r['mae']:.2f}, RMSE={r['rmse']:.2f} (Best Epoch: {r['best_epoch']})\")\n",
    "\n",
    "    best_fold_result = max(cv_results, key=lambda x: x['r2'])\n",
    "    print(f\"\\nBest fold: {best_fold_result['fold']} (R2={best_fold_result['r2']:.4f})\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TEST SET EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    train_full, test_full, final_feature_cols = engineer_features_for_fold(\n",
    "        train_df_raw.drop('bmi_bin', axis=1), test_df=test_df_raw\n",
    "    )\n",
    "\n",
    "    final_model = HybridModel(\n",
    "        len(final_feature_cols), num_landmarks, landmark_dim, 0.3, True\n",
    "    ).to(device)\n",
    "    final_model.load_state_dict(best_fold_result['state'])\n",
    "\n",
    "    test_ds = BMIDataset(\n",
    "        test_full, final_feature_cols, landmark_info, IMAGE_DIR,\n",
    "        is_training=False, augment_mult=1,\n",
    "        feature_scaler=best_fold_result['feature_scaler'],\n",
    "        landmark_scaler=best_fold_result['landmark_scaler'],\n",
    "        fit_scalers=False\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_ds, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers, pin_memory=device.type=='cuda',\n",
    "        collate_fn=collate_fn, persistent_workers=False\n",
    "    )\n",
    "\n",
    "    edges_device = {k: v.to(device) for k, v in edges.items()} if edges else None\n",
    "    criterion = MultiTaskLoss()\n",
    "\n",
    "    _, _, test_r2, test_mae, test_preds, test_targets = validate_epoch(\n",
    "        final_model, test_loader, criterion, device, edges_device\n",
    "    )\n",
    "    test_rmse = np.sqrt(mean_squared_error(test_targets, test_preds))\n",
    "    residuals = np.array(test_targets) - np.array(test_preds)\n",
    "\n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"  R2:   {test_r2:.4f}\")\n",
    "    print(f\"  MAE:  {test_mae:.2f}\")\n",
    "    print(f\"  RMSE: {test_rmse:.2f}\")\n",
    "    print(f\"  Mean Residual: {residuals.mean():.2f} ¬± {residuals.std():.2f}\")\n",
    "\n",
    "    print(f\"\\nPrediction Accuracy:\")\n",
    "    for t in [1, 2, 3, 5]:\n",
    "        pct = np.mean(np.abs(residuals) < t) * 100\n",
    "        print(f\"  Within ¬±{t} BMI: {pct:.1f}%\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SAVING RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    save_path = os.path.join(OUTPUT_DIR, 'hybrid_model_v2.pth')\n",
    "    torch.save({\n",
    "        'model_state': best_fold_result['state'],\n",
    "        'feature_scaler': best_fold_result['feature_scaler'],\n",
    "        'landmark_scaler': best_fold_result['landmark_scaler'],\n",
    "        'feature_cols': final_feature_cols,\n",
    "        'num_landmarks': num_landmarks,\n",
    "        'landmark_dim': landmark_dim,\n",
    "        'graph_edges': edges,\n",
    "        'cv_results': [{k: v for k, v in r.items() if k not in ['state', 'feature_scaler', 'landmark_scaler']}\n",
    "                       for r in cv_results],\n",
    "        'test_results': {'r2': test_r2, 'mae': test_mae, 'rmse': test_rmse}\n",
    "    }, save_path)\n",
    "    print(f\"‚úì Model saved: {save_path}\")\n",
    "\n",
    "    pred_df = pd.DataFrame({\n",
    "        'image_filename': test_df_raw['image_filename'].values,\n",
    "        'actual_bmi': test_targets,\n",
    "        'predicted_bmi': test_preds,\n",
    "        'residual': residuals\n",
    "    })\n",
    "    pred_path = os.path.join(OUTPUT_DIR, 'predictions_v2.csv')\n",
    "    pred_df.to_csv(pred_path, index=False)\n",
    "    print(f\"‚úì Predictions saved: {pred_path}\")\n",
    "\n",
    "    edges_path = os.path.join(OUTPUT_DIR, 'graph_edges.pth')\n",
    "    torch.save(edges, edges_path)\n",
    "    print(f\"‚úì Graph edges saved: {edges_path}\")\n",
    "\n",
    "    results_dict = {\n",
    "        'cv_results': cv_results,\n",
    "        'test_targets': test_targets,\n",
    "        'test_preds': test_preds,\n",
    "        'test_r2': test_r2,\n",
    "        'test_mae': test_mae,\n",
    "        'test_rmse': test_rmse,\n",
    "        'residuals': residuals\n",
    "    }\n",
    "    \n",
    "    import pickle\n",
    "    with open(os.path.join(OUTPUT_DIR, 'results_for_viz.pkl'), 'wb') as f:\n",
    "        pickle.dump(results_dict, f)\n",
    "    \n",
    "    print(f\"‚úì Results saved for visualization\")\n",
    "\n",
    "    del final_model, test_ds, test_loader\n",
    "    clear_memory()\n",
    "\n",
    "    return results_dict\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T16:40:51.773544Z",
     "iopub.status.busy": "2026-01-11T16:40:51.772918Z",
     "iopub.status.idle": "2026-01-11T16:40:51.777448Z",
     "shell.execute_reply": "2026-01-11T16:40:51.776899Z",
     "shell.execute_reply.started": "2026-01-11T16:40:51.773517Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Install required libraries\n",
    "!pip install pillow -q\n",
    "\n",
    "# Step 2: Import necessary modules\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Step 3: Create output directory\n",
    "# Use the actual Kaggle input path (this should already exist as a dataset)\n",
    "input_dir = '/kaggle/input/vip-attribute/data/data'\n",
    "output_dir = '/kaggle/working/processed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Step 4: Check if input directory exists\n",
    "if not os.path.exists(input_dir):\n",
    "    print(f\"ERROR: Input directory '{input_dir}' does not exist!\")\n",
    "    print(\"Please check your dataset path.\")\n",
    "else:\n",
    "    print(f\"Input directory found: {input_dir}\")\n",
    "    \n",
    "    # Step 5: Process all images in the input folder\n",
    "    target_size = (224, 224)\n",
    "    min_size = 200\n",
    "    \n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    image_files = [f for f in os.listdir(input_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    print(f\"Found {len(image_files)} image files\")\n",
    "    \n",
    "    for filename in image_files:\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        try:\n",
    "            img = Image.open(input_path).convert(\"RGB\")\n",
    "            width, height = img.size\n",
    "            \n",
    "            # Skip if either dimension is below 200\n",
    "            if width < min_size or height < min_size:\n",
    "                \n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Resize the image while preserving aspect ratio\n",
    "            img.thumbnail(target_size, Image.LANCZOS)\n",
    "            \n",
    "            # Create a solid black background\n",
    "            final_img = Image.new(\"RGB\", target_size, (0, 0, 0))\n",
    "            \n",
    "            # Center the image on the black background\n",
    "            offset = ((target_size[0] - img.size[0]) // 2, (target_size[1] - img.size[1]) // 2)\n",
    "            final_img.paste(img, offset)\n",
    "            \n",
    "            # Save as JPEG\n",
    "            output_filename = os.path.splitext(filename)[0] + '.jpg'\n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "            final_img.save(output_path, \"JPEG\", quality=95)\n",
    "            \n",
    "            processed_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            error_count += 1\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing complete!\")\n",
    "    print(f\"Processed: {processed_count} images\")\n",
    "    print(f\"Skipped: {skipped_count} images (too small)\")\n",
    "    print(f\"Errors: {error_count} images\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T16:40:51.778743Z",
     "iopub.status.busy": "2026-01-11T16:40:51.778446Z",
     "iopub.status.idle": "2026-01-11T16:40:51.798045Z",
     "shell.execute_reply": "2026-01-11T16:40:51.797348Z",
     "shell.execute_reply.started": "2026-01-11T16:40:51.778723Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (change the filename if needed)\n",
    "df = pd.read_csv(\"/kaggle/input/bmi-dataset/test.csv\")\n",
    "\n",
    "# Drop the height and weight columns\n",
    "df = df.drop(columns=[\"height\", \"weight\"])\n",
    "\n",
    "# Save the updated dataset to Kaggle working directory\n",
    "output_path = \"/kaggle/working/cleaned_dataset.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "# \n",
    "# print(f\"File saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T16:40:51.864324Z",
     "iopub.status.busy": "2026-01-11T16:40:51.863769Z",
     "iopub.status.idle": "2026-01-11T16:40:51.870779Z",
     "shell.execute_reply": "2026-01-11T16:40:51.870142Z",
     "shell.execute_reply.started": "2026-01-11T16:40:51.864296Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set memory-efficient configurations\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "DATA_PATH = '/kaggle/working/cleaned_dataset.csv'\n",
    "IMAGE_DIR = '/kaggle/input/bmi-dataset/ROI'\n",
    "OUTPUT_DIR = './shap_analysis_lightweight'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# DATA LOADING AND PREPARATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LIGHTWEIGHT SHAP ANALYSIS FOR BMI PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load the dataset\n",
    "print(f\"\\n1. Loading dataset from: {DATA_PATH}\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\"   ‚úì Loaded {len(df)} samples with {len(df.columns)} columns\")\n",
    "\n",
    "if 'BMI' in df.columns:\n",
    "    print(f\"   ‚úì BMI column found: {df['BMI'].min():.1f} to {df['BMI'].max():.1f}\")\n",
    "else:\n",
    "    print(\"   ‚ö† BMI column not found, creating synthetic BMI\")\n",
    "    df['BMI'] = np.random.uniform(18, 35, len(df))\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n3. Preparing features...\")\n",
    "exclude_cols = ['id', 'image_filename', 'image', 'BMI', 'ID', 'filename']\n",
    "feature_cols = [col for col in df.columns \n",
    "                if col not in exclude_cols \n",
    "                and pd.api.types.is_numeric_dtype(df[col])]\n",
    "\n",
    "print(f\"   ‚úì Selected {len(feature_cols)} numeric features\")\n",
    "\n",
    "MAX_SAMPLES = 100\n",
    "if len(df) > MAX_SAMPLES:\n",
    "    df_sample = df.sample(MAX_SAMPLES, random_state=42)\n",
    "    print(f\"   ‚ö† Using subset of {MAX_SAMPLES} samples\")\n",
    "else:\n",
    "    df_sample = df\n",
    "\n",
    "X = df_sample[feature_cols].fillna(0).astype(np.float32)\n",
    "y = df_sample['BMI'].values\n",
    "\n",
    "print(f\"   ‚úì X shape: {X.shape}, y range: {y.min():.1f} to {y.max():.1f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# TRAIN MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n4. Training model...\")\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=50, max_depth=10, min_samples_split=5, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"   ‚úì Model - MAE: {mae:.4f}, R¬≤: {r2:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# FIXED SHAP ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n5. SHAP analysis with FIXED plots...\")\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    \n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_sample_size = min(50, len(X_test_scaled))\n",
    "    X_sample = X_test_scaled[:shap_sample_size]\n",
    "    shap_values = explainer.shap_values(X_sample)\n",
    "    \n",
    "    print(f\"   ‚úì SHAP values for {shap_sample_size} samples\")\n",
    "    \n",
    "    # FIXED SUMMARY PLOT\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    shap.summary_plot(shap_values, X_sample, feature_names=feature_cols, \n",
    "                     show=False, max_display=15)\n",
    "    plt.title('SHAP Feature Importance for BMI Prediction', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'shap_summary.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # FIXED BAR PLOT - COMPLETE X-AXIS LABEL\n",
    "    plt.figure(figsize=(16, 12))  # Larger size\n",
    "    shap.summary_plot(shap_values, X_sample, feature_names=feature_cols,\n",
    "                     plot_type=\"bar\", show=False, max_display=20)\n",
    "    \n",
    "    # Customize to prevent cutoff\n",
    "    plt.title('Mean Absolute SHAP Values (Impact on Model Output)', \n",
    "              fontsize=18, fontweight='bold', pad=25)\n",
    "    plt.xlabel('Mean |SHAP value| (average impact on model output)', fontsize=14)\n",
    "    plt.ylabel('Features', fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=11)\n",
    "    \n",
    "    # CRITICAL: Adjust margins to prevent x-axis label cutoff\n",
    "    plt.subplots_adjust(left=0.18, right=0.98, top=0.90, bottom=0.15, hspace=0.3)\n",
    "    \n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'shap_bar.png'), \n",
    "                dpi=200, bbox_inches='tight', facecolor='white')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"   ‚úì FIXED bar plot saved - full x-axis label visible!\")\n",
    "    \n",
    "    # Feature importance table\n",
    "    mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Mean_abs_SHAP': mean_abs_shap,\n",
    "        'Rank': np.argsort(-mean_abs_shap) + 1\n",
    "    }).sort_values('Mean_abs_SHAP', ascending=False)\n",
    "    \n",
    "    importance_df.to_csv(os.path.join(OUTPUT_DIR, 'feature_importance.csv'), index=False)\n",
    "    \n",
    "    # TOP FEATURES HORIZONTAL BAR CHART\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    top_n = min(15, len(importance_df))\n",
    "    top_features = importance_df.head(top_n)\n",
    "    \n",
    "    bars = plt.barh(range(top_n), top_features['Mean_abs_SHAP'][::-1], \n",
    "                    color='steelblue', alpha=0.8, edgecolor='navy', linewidth=0.5)\n",
    "    plt.yticks(range(top_n), top_features['Feature'][::-1], fontsize=11)\n",
    "    plt.xlabel('Mean Absolute SHAP Value', fontsize=14, fontweight='bold')\n",
    "    plt.title(f'Top {top_n} Most Important Features for BMI Prediction', \n",
    "              fontsize=16, fontweight='bold')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'top_features.png'), dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"\\n‚úÖ SHAP analysis COMPLETE!\")\n",
    "    print(f\"üìÅ Files saved to: {OUTPUT_DIR}\")\n",
    "    print(\"\\nüèÜ TOP 5 FEATURES:\")\n",
    "    for i, row in enumerate(importance_df.head(5).itertuples(), 1):\n",
    "        print(f\"{i:2d}. {row.Feature:35s} | SHAP: {row.Mean_abs_SHAP:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† SHAP failed: {e}\")\n",
    "    print(\"Using model feature importance...\")\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Importance': model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    feature_importance.to_csv(os.path.join(OUTPUT_DIR, 'feature_importance.csv'), index=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    top_n = 15\n",
    "    plt.barh(range(top_n), feature_importance['Importance'].head(top_n)[::-1], \n",
    "             color='steelblue', alpha=0.8)\n",
    "    plt.yticks(range(top_n), feature_importance['Feature'].head(top_n)[::-1])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Top Features (Model Built-in Importance)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'feature_importance.png'), dpi=200, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "print(f\"\\nüéâ Analysis complete! Check '{OUTPUT_DIR}' for all plots and CSV files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T16:40:52.369899Z",
     "iopub.status.busy": "2026-01-11T16:40:52.369384Z",
     "iopub.status.idle": "2026-01-11T16:40:52.396326Z",
     "shell.execute_reply": "2026-01-11T16:40:52.395497Z",
     "shell.execute_reply.started": "2026-01-11T16:40:52.369863Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# FAST ABLATION CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# üöÄ SPEED OPTIMIZATIONS\n",
    "FAST_MODE = True  # Set to False for full training\n",
    "SAMPLE_SIZE = 5000  # Use only 500 samples\n",
    "NUM_EPOCHS = 10    # Reduced from 15\n",
    "PATIENCE = 3      # Reduced from 5\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"‚ö° FAST ABLATION MODE ENABLED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üìä Sample Size: {SAMPLE_SIZE} (vs full dataset)\")\n",
    "print(f\"üîÑ Max Epochs: {NUM_EPOCHS} (vs 15)\")\n",
    "print(f\"‚è±Ô∏è  Patience: {PATIENCE} (vs 5)\")\n",
    "print(f\"üéØ Expected Time: ~5-15 minutes (vs 12 hours)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# ABLATION STUDY CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "ABLATION_CONFIGS = {\n",
    "    'FULL_MODEL': {\n",
    "        'use_images': True,\n",
    "        'use_tabular': True,\n",
    "        'use_augmentation': True,\n",
    "        'use_derived_features': True,\n",
    "        'use_dropout': True,\n",
    "        'dropout_rate': 0.3,\n",
    "        'use_batch_norm': True,\n",
    "        'use_pretrained': True,\n",
    "        'augment_multiplier': 3,  # Changed from 3\n",
    "        'feature_noise': 0.02,\n",
    "        'description': 'Full multimodal model with all features'\n",
    "    },\n",
    "    'NO_IMAGES': {\n",
    "        'use_images': False,\n",
    "        'use_tabular': True,\n",
    "        'use_augmentation': True,\n",
    "        'use_derived_features': True,\n",
    "        'use_dropout': True,\n",
    "        'dropout_rate': 0.3,\n",
    "        'use_batch_norm': True,\n",
    "        'use_pretrained': True,\n",
    "        'augment_multiplier': 3,  # Changed from 3\n",
    "        'feature_noise': 0.02,\n",
    "        'description': 'Tabular features only (no images)'\n",
    "    },\n",
    "    'NO_TABULAR': {\n",
    "        'use_images': True,\n",
    "        'use_tabular': False,\n",
    "        'use_augmentation': True,\n",
    "        'use_derived_features': False,\n",
    "        'use_dropout': True,\n",
    "        'dropout_rate': 0.3,\n",
    "        'use_batch_norm': True,\n",
    "        'use_pretrained': True,\n",
    "        'augment_multiplier': 3,  # Changed from 3\n",
    "        'feature_noise': 0.0,\n",
    "        'description': 'Images only (no tabular features)'\n",
    "    },\n",
    "    'NO_AUGMENTATION': {\n",
    "        'use_images': True,\n",
    "        'use_tabular': True,\n",
    "        'use_augmentation': False,\n",
    "        'use_derived_features': True,\n",
    "        'use_dropout': True,\n",
    "        'dropout_rate': 0.3,\n",
    "        'use_batch_norm': True,\n",
    "        'use_pretrained': True,\n",
    "        'augment_multiplier': 3,\n",
    "        'feature_noise': 0.0,\n",
    "        'description': 'No data augmentation'\n",
    "    },\n",
    "    'NO_DERIVED_FEATURES': {\n",
    "        'use_images': True,\n",
    "        'use_tabular': True,\n",
    "        'use_augmentation': True,\n",
    "        'use_derived_features': False,\n",
    "        'use_dropout': True,\n",
    "        'dropout_rate': 0.3,\n",
    "        'use_batch_norm': True,\n",
    "        'use_pretrained': True,\n",
    "        'augment_multiplier': 2,  # Changed from 3\n",
    "        'feature_noise': 0.02,\n",
    "        'description': 'No engineered/derived features'\n",
    "    },\n",
    "    'NO_DROPOUT': {\n",
    "        'use_images': True,\n",
    "        'use_tabular': True,\n",
    "        'use_augmentation': True,\n",
    "        'use_derived_features': True,\n",
    "        'use_dropout': False,\n",
    "        'dropout_rate': 0.0,\n",
    "        'use_batch_norm': True,\n",
    "        'use_pretrained': True,\n",
    "        'augment_multiplier': 3,  # Changed from 3\n",
    "        'feature_noise': 0.02,\n",
    "        'description': 'No dropout regularization'\n",
    "    },\n",
    "    'HIGH_DROPOUT': {\n",
    "        'use_images': True,\n",
    "        'use_tabular': True,\n",
    "        'use_augmentation': True,\n",
    "        'use_derived_features': True,\n",
    "        'use_dropout': True,\n",
    "        'dropout_rate': 0.5,\n",
    "        'use_batch_norm': True,\n",
    "        'use_pretrained': True,\n",
    "        'augment_multiplier': 3,  # Changed from 3\n",
    "        'feature_noise': 0.02,\n",
    "        'description': 'High dropout (0.5 instead of 0.3)'\n",
    "    },\n",
    "    'NO_BATCH_NORM': {\n",
    "        'use_images': True,\n",
    "        'use_tabular': True,\n",
    "        'use_augmentation': True,\n",
    "        'use_derived_features': True,\n",
    "        'use_dropout': True,\n",
    "        'dropout_rate': 0.3,\n",
    "        'use_batch_norm': False,\n",
    "        'use_pretrained': True,\n",
    "        'augment_multiplier': 3,  # Changed from 3\n",
    "        'feature_noise': 0.02,\n",
    "        'description': 'No batch normalization'\n",
    "    },\n",
    "    'NO_PRETRAINED': {\n",
    "        'use_images': True,\n",
    "        'use_tabular': True,\n",
    "        'use_augmentation': True,\n",
    "        'use_derived_features': True,\n",
    "        'use_dropout': True,\n",
    "        'dropout_rate': 0.3,\n",
    "        'use_batch_norm': True,\n",
    "        'use_pretrained': False,\n",
    "        'augment_multiplier': 3,  # Changed from 3\n",
    "        'feature_noise': 0.02,\n",
    "        'description': 'Random initialization (no pretrained weights)'\n",
    "    },\n",
    "    'MINIMAL_MODEL': {\n",
    "        'use_images': True,\n",
    "        'use_tabular': True,\n",
    "        'use_augmentation': False,\n",
    "        'use_derived_features': False,\n",
    "        'use_dropout': False,\n",
    "        'dropout_rate': 0.0,\n",
    "        'use_batch_norm': False,\n",
    "        'use_pretrained': False,\n",
    "        'augment_multiplier': 3,\n",
    "        'feature_noise': 0.0,\n",
    "        'description': 'Minimal model (basic features, no regularization)'\n",
    "    }\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP\n",
    "# ============================================================================\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Paths (adjust these to your environment)\n",
    "DATA_DIR = '/kaggle/input/bmi-dataset'\n",
    "TRAIN_CSV = '/kaggle/input/bmi-dataset/train.csv'\n",
    "TEST_CSV = '/kaggle/input/bmi-dataset/test.csv'\n",
    "IMAGE_DIR = '/kaggle/input/bmi-dataset/ROI'\n",
    "\n",
    "print(\"üî¨ ABLATION STUDY - BMI PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    train_df = pd.read_csv(TRAIN_CSV)\n",
    "    test_df = pd.read_csv(TEST_CSV)\n",
    "    print(f\"‚úÖ Data loaded: Train={train_df.shape}, Test={test_df.shape}\")\n",
    "except:\n",
    "    print(\"‚ùå Error loading data. Using synthetic data for demonstration.\")\n",
    "    # Create synthetic data\n",
    "    n_train, n_test = 1000, 200\n",
    "    train_df = pd.DataFrame({\n",
    "        'BMI': np.random.normal(25, 5, n_train),\n",
    "        'age': np.random.randint(18, 80, n_train),\n",
    "        'sex_encoded': np.random.randint(0, 2, n_train),\n",
    "        'face_height': np.random.normal(100, 10, n_train),\n",
    "        'face_width_cheeks': np.random.normal(80, 8, n_train),\n",
    "        'face_width_jaw': np.random.normal(75, 8, n_train),\n",
    "        'left_eye_width': np.random.normal(20, 2, n_train),\n",
    "        'right_eye_width': np.random.normal(20, 2, n_train),\n",
    "        'nose_length': np.random.normal(30, 3, n_train),\n",
    "        'nose_width': np.random.normal(25, 2.5, n_train),\n",
    "        'image_filename': [f'img_{i}.jpg' for i in range(n_train)]\n",
    "    })\n",
    "    test_df = train_df.copy().sample(n_test)\n",
    "    IMAGE_DIR = None\n",
    "\n",
    "# üöÄ SAMPLE DATA FOR SPEED\n",
    "if FAST_MODE and len(train_df) > SAMPLE_SIZE:\n",
    "    print(f\"\\n‚ö° Fast Mode: Sampling {SAMPLE_SIZE} rows from {len(train_df)}\")\n",
    "    train_df = train_df.sample(SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n",
    "    print(f\"‚úÖ Using {len(train_df)} samples for ablation study\")\n",
    "\n",
    "# Drop height/weight if present\n",
    "for col in ['height', 'weight']:\n",
    "    if col in train_df.columns:\n",
    "        train_df = train_df.drop(columns=[col])\n",
    "        test_df = test_df.drop(columns=[col])\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "def prepare_features(df, include_derived=True):\n",
    "    \"\"\"Prepare features with optional derived features.\"\"\"\n",
    "    df_eng = df.copy()\n",
    "    \n",
    "    # Basic features\n",
    "    basic_features = ['face_height', 'face_width_cheeks', 'face_width_jaw',\n",
    "                      'left_eye_width', 'right_eye_width', 'nose_length', \n",
    "                      'nose_width', 'age', 'sex_encoded']\n",
    "    \n",
    "    available_features = [f for f in basic_features if f in df_eng.columns]\n",
    "    \n",
    "    if include_derived:\n",
    "        # Derived features\n",
    "        if 'face_height' in df_eng.columns and 'face_width_cheeks' in df_eng.columns:\n",
    "            df_eng['fwhr'] = (df_eng['face_width_cheeks'] / \n",
    "                             df_eng['face_height'].clip(lower=1e-8)).fillna(0)\n",
    "            available_features.append('fwhr')\n",
    "        \n",
    "        if 'left_eye_width' in df_eng.columns and 'right_eye_width' in df_eng.columns:\n",
    "            sum_eyes = df_eng['left_eye_width'] + df_eng['right_eye_width']\n",
    "            df_eng['eye_symmetry'] = 1 - abs(df_eng['left_eye_width'] - \n",
    "                                             df_eng['right_eye_width']) / sum_eyes.clip(lower=1e-8)\n",
    "            df_eng['eye_symmetry'] = df_eng['eye_symmetry'].fillna(0)\n",
    "            available_features.append('eye_symmetry')\n",
    "        \n",
    "        if 'face_width_cheeks' in df_eng.columns and 'face_height' in df_eng.columns:\n",
    "            df_eng['face_area'] = (df_eng['face_width_cheeks'] * \n",
    "                                  df_eng['face_height']).fillna(0)\n",
    "            available_features.append('face_area')\n",
    "    \n",
    "    return df_eng, available_features\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class AblationDataset(Dataset):\n",
    "    \"\"\"Dataset for ablation study.\"\"\"\n",
    "    def __init__(self, df, feature_cols, image_dir, config, is_training=True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.feature_cols = feature_cols\n",
    "        self.image_dir = image_dir\n",
    "        self.config = config\n",
    "        self.is_training = is_training\n",
    "        \n",
    "        if config['use_tabular']:\n",
    "            self.features = df[feature_cols].values.astype(np.float32)\n",
    "        self.targets = df['BMI'].values.astype(np.float32)\n",
    "        \n",
    "        if 'image_filename' in df.columns:\n",
    "            self.filenames = df['image_filename'].tolist()\n",
    "        else:\n",
    "            self.filenames = [f'img_{i}.jpg' for i in range(len(df))]\n",
    "        \n",
    "        # Setup transforms\n",
    "        if config['use_images']:\n",
    "            if is_training and config['use_augmentation']:\n",
    "                self.transform = transforms.Compose([\n",
    "                    transforms.Resize((256, 256)),\n",
    "                    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "                    transforms.RandomHorizontalFlip(p=0.5),\n",
    "                    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                       std=[0.229, 0.224, 0.225])\n",
    "                ])\n",
    "            else:\n",
    "                self.transform = transforms.Compose([\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                       std=[0.229, 0.224, 0.225])\n",
    "                ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.is_training and self.config['augment_multiplier'] > 1:\n",
    "            return len(self.df) * self.config['augment_multiplier']\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        original_idx = idx % len(self.df)\n",
    "        is_augmented = idx >= len(self.df)\n",
    "        \n",
    "        target = torch.tensor(self.targets[original_idx], dtype=torch.float32)\n",
    "        \n",
    "        # Tabular features\n",
    "        if self.config['use_tabular']:\n",
    "            features = torch.tensor(self.features[original_idx], dtype=torch.float32)\n",
    "            if self.is_training and is_augmented and self.config['feature_noise'] > 0:\n",
    "                noise = torch.randn_like(features) * self.config['feature_noise']\n",
    "                features = features + noise\n",
    "        else:\n",
    "            features = torch.zeros(1)  # Dummy\n",
    "        \n",
    "        # Image\n",
    "        if self.config['use_images'] and self.image_dir is not None:\n",
    "            filename = self.filenames[original_idx]\n",
    "            img_path = os.path.join(self.image_dir, filename)\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    if img.mode != 'RGB':\n",
    "                        img = img.convert('RGB')\n",
    "                    image = self.transform(img)\n",
    "            except:\n",
    "                image = torch.zeros(3, 224, 224)\n",
    "        else:\n",
    "            # Dummy image\n",
    "            image = torch.randn(3, 224, 224) * 0.1\n",
    "        \n",
    "        return {\n",
    "            'features': features,\n",
    "            'image': image,\n",
    "            'target': target\n",
    "        }\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL ARCHITECTURE\n",
    "# ============================================================================\n",
    "\n",
    "class AblationModel(nn.Module):\n",
    "    \"\"\"Model for ablation study.\"\"\"\n",
    "    def __init__(self, num_features, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Image branch\n",
    "        if config['use_images']:\n",
    "            if config['use_pretrained']:\n",
    "                resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "            else:\n",
    "                resnet = models.resnet18(weights=None)\n",
    "            \n",
    "            for param in resnet.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.image_features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "            image_dim = 512\n",
    "        else:\n",
    "            image_dim = 0\n",
    "        \n",
    "        # Tabular branch\n",
    "        if config['use_tabular']:\n",
    "            layers = []\n",
    "            layers.append(nn.Linear(num_features, 128))\n",
    "            if config['use_batch_norm']:\n",
    "                layers.append(nn.BatchNorm1d(128))\n",
    "            layers.append(nn.ReLU())\n",
    "            if config['use_dropout']:\n",
    "                layers.append(nn.Dropout(config['dropout_rate']))\n",
    "            \n",
    "            layers.append(nn.Linear(128, 64))\n",
    "            if config['use_batch_norm']:\n",
    "                layers.append(nn.BatchNorm1d(64))\n",
    "            layers.append(nn.ReLU())\n",
    "            \n",
    "            self.tabular_features = nn.Sequential(*layers)\n",
    "            tabular_dim = 64\n",
    "        else:\n",
    "            tabular_dim = 0\n",
    "        \n",
    "        # Fusion\n",
    "        combined_dim = image_dim + tabular_dim\n",
    "        \n",
    "        if combined_dim == 0:\n",
    "            raise ValueError(\"Must use at least images or tabular features!\")\n",
    "        \n",
    "        fusion_layers = []\n",
    "        fusion_layers.append(nn.Linear(combined_dim, 128))\n",
    "        if config['use_batch_norm']:\n",
    "            fusion_layers.append(nn.BatchNorm1d(128))\n",
    "        fusion_layers.append(nn.ReLU())\n",
    "        if config['use_dropout']:\n",
    "            fusion_layers.append(nn.Dropout(config['dropout_rate']))\n",
    "        \n",
    "        fusion_layers.append(nn.Linear(128, 64))\n",
    "        if config['use_batch_norm']:\n",
    "            fusion_layers.append(nn.BatchNorm1d(64))\n",
    "        fusion_layers.append(nn.ReLU())\n",
    "        \n",
    "        self.fusion = nn.Sequential(*fusion_layers)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, features, image):\n",
    "        batch_size = features.size(0)\n",
    "        feat_list = []\n",
    "        \n",
    "        if self.config['use_images']:\n",
    "            img_feat = self.image_features(image)\n",
    "            img_feat = img_feat.view(batch_size, -1)\n",
    "            feat_list.append(img_feat)\n",
    "        \n",
    "        if self.config['use_tabular']:\n",
    "            tab_feat = self.tabular_features(features)\n",
    "            feat_list.append(tab_feat)\n",
    "        \n",
    "        combined = torch.cat(feat_list, dim=1)\n",
    "        fused = self.fusion(combined)\n",
    "        output = self.output(fused)\n",
    "        \n",
    "        return output.squeeze()\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def train_model(config, train_df, val_df, feature_cols, device, \n",
    "                num_epochs=NUM_EPOCHS, patience=PATIENCE):\n",
    "    \"\"\"Train model with given configuration.\"\"\"\n",
    "    \n",
    "    # Prepare features\n",
    "    if config['use_derived_features']:\n",
    "        train_eng, train_features = prepare_features(train_df, include_derived=True)\n",
    "        val_eng, _ = prepare_features(val_df, include_derived=True)\n",
    "    else:\n",
    "        train_eng, train_features = prepare_features(train_df, include_derived=False)\n",
    "        val_eng, _ = prepare_features(val_df, include_derived=False)\n",
    "    \n",
    "    # Use provided feature_cols or computed ones\n",
    "    if config['use_tabular']:\n",
    "        use_features = [f for f in train_features if f in train_eng.columns]\n",
    "    else:\n",
    "        use_features = ['age']  # Dummy\n",
    "    \n",
    "    # Scale features\n",
    "    if config['use_tabular']:\n",
    "        scaler = RobustScaler()\n",
    "        train_eng[use_features] = scaler.fit_transform(\n",
    "            train_eng[use_features].fillna(0).values)\n",
    "        val_eng[use_features] = scaler.transform(\n",
    "            val_eng[use_features].fillna(0).values)\n",
    "    else:\n",
    "        scaler = None\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = AblationDataset(train_eng, use_features, IMAGE_DIR, \n",
    "                                    config, is_training=True)\n",
    "    val_dataset = AblationDataset(val_eng, use_features, IMAGE_DIR, \n",
    "                                  config, is_training=False)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, \n",
    "                             num_workers=2)  # Changed from 0 to 2\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, \n",
    "                           num_workers=2)  # Changed from 0 to 2\n",
    "    \n",
    "    # Create model\n",
    "    model = AblationModel(len(use_features) if config['use_tabular'] else 1, \n",
    "                         config).to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                           lr=1e-3, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                     factor=0.5, patience=2)  # Reduced patience\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_r2': []}\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            features = batch['features'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features, images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds, val_targets = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                features = batch['features'].to(device)\n",
    "                images = batch['image'].to(device)\n",
    "                targets = batch['target'].to(device)\n",
    "                \n",
    "                outputs = model(features, images)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                val_preds.extend(outputs.cpu().numpy())\n",
    "                val_targets.extend(targets.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_r2 = r2_score(val_targets, val_preds)\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_r2'].append(val_r2)\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"   Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Final validation\n",
    "    model.eval()\n",
    "    val_preds, val_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            features = batch['features'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            outputs = model(features, images)\n",
    "            val_preds.extend(outputs.cpu().numpy())\n",
    "            val_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    final_r2 = r2_score(val_targets, val_preds)\n",
    "    final_mae = mean_absolute_error(val_targets, val_preds)\n",
    "    final_rmse = np.sqrt(mean_squared_error(val_targets, val_preds))\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'scaler': scaler,\n",
    "        'r2': final_r2,\n",
    "        'mae': final_mae,\n",
    "        'rmse': final_rmse,\n",
    "        'history': history,\n",
    "        'use_features': use_features\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# RUN ABLATION STUDY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ RUNNING FAST ABLATION STUDY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\\n\")\n",
    "\n",
    "# Use simple train/val split for speed\n",
    "train_idx, val_idx = train_test_split(range(len(train_df)), test_size=0.2, \n",
    "                                       random_state=42)\n",
    "train_split = train_df.iloc[train_idx].copy()\n",
    "val_split = train_df.iloc[val_idx].copy()\n",
    "\n",
    "print(f\"Train samples: {len(train_split)}, Val samples: {len(val_split)}\\n\")\n",
    "\n",
    "ablation_results = {}\n",
    "import time\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "for i, (config_name, config) in enumerate(ABLATION_CONFIGS.items(), 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üß™ [{i}/{len(ABLATION_CONFIGS)}] {config_name}\")\n",
    "    print(f\"üìù {config['description']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    config_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        result = train_model(\n",
    "            config, train_split, val_split, \n",
    "            feature_cols=None,  # Will be computed inside\n",
    "            device=device,\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            patience=PATIENCE\n",
    "        )\n",
    "        \n",
    "        config_time = time.time() - config_start\n",
    "        \n",
    "        ablation_results[config_name] = result\n",
    "        \n",
    "        print(f\"‚úÖ Results (trained in {config_time:.1f}s):\")\n",
    "        print(f\"   R¬≤: {result['r2']:.4f}\")\n",
    "        print(f\"   MAE: {result['mae']:.2f}\")\n",
    "        print(f\"   RMSE: {result['rmse']:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed: {str(e)}\")\n",
    "        ablation_results[config_name] = None\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"\\n‚è±Ô∏è  Total training time: {total_time/60:.1f} minutes\")\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä CREATING ABLATION VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Filter successful results\n",
    "successful_results = {k: v for k, v in ablation_results.items() if v is not None}\n",
    "\n",
    "if len(successful_results) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. R¬≤ Comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    names = list(successful_results.keys())\n",
    "    r2_scores = [successful_results[n]['r2'] for n in names]\n",
    "    colors = ['red' if n == 'FULL_MODEL' else 'skyblue' for n in names]\n",
    "    bars = ax1.barh(names, r2_scores, color=colors, edgecolor='black')\n",
    "    ax1.set_xlabel('R¬≤ Score', fontsize=12)\n",
    "    ax1.set_title('R¬≤ Score by Configuration', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3, axis='x')\n",
    "    for bar, score in zip(bars, r2_scores):\n",
    "        width = bar.get_width()\n",
    "        ax1.text(width + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                f'{score:.3f}', va='center', fontsize=10)\n",
    "    \n",
    "    # 2. MAE Comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    mae_scores = [successful_results[n]['mae'] for n in names]\n",
    "    colors = ['red' if n == 'FULL_MODEL' else 'coral' for n in names]\n",
    "    bars = ax2.barh(names, mae_scores, color=colors, edgecolor='black')\n",
    "    ax2.set_xlabel('MAE (kg/m¬≤)', fontsize=12)\n",
    "    ax2.set_title('MAE by Configuration', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='x')\n",
    "    for bar, score in zip(bars, mae_scores):\n",
    "        width = bar.get_width()\n",
    "        ax2.text(width + 0.05, bar.get_y() + bar.get_height()/2, \n",
    "                f'{score:.2f}', va='center', fontsize=10)\n",
    "    \n",
    "    # 3. Performance Degradation from Full Model\n",
    "    ax3 = axes[1, 0]\n",
    "    full_r2 = successful_results['FULL_MODEL']['r2']\n",
    "    degradation = [(full_r2 - successful_results[n]['r2']) * 100 for n in names]\n",
    "    colors = ['green' if d <= 0 else 'red' for d in degradation]\n",
    "    bars = ax3.barh(names, degradation, color=colors, edgecolor='black', alpha=0.7)\n",
    "    ax3.set_xlabel('R¬≤ Degradation (%)', fontsize=12)\n",
    "    ax3.set_title('Performance Drop vs Full Model', fontsize=14, fontweight='bold')\n",
    "    ax3.axvline(x=0, color='black', linestyle='--', linewidth=2)\n",
    "    ax3.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # 4. Training History (Full Model)\n",
    "    ax4 = axes[1, 1]\n",
    "    if 'FULL_MODEL' in successful_results:\n",
    "        history = successful_results['FULL_MODEL']['history']\n",
    "        epochs = range(1, len(history['train_loss']) + 1)\n",
    "        ax4.plot(epochs, history['train_loss'], label='Train Loss', linewidth=2)\n",
    "        ax4.plot(epochs, history['val_loss'], label='Val Loss', linewidth=2)\n",
    "        ax4_twin = ax4.twinx()\n",
    "        ax4_twin.plot(epochs, history['val_r2'], label='Val R¬≤', \n",
    "                     color='green', linewidth=2, linestyle='--')\n",
    "        ax4.set_xlabel('Epoch', fontsize=12)\n",
    "        ax4.set_ylabel('Loss', fontsize=12)\n",
    "        ax4_twin.set_ylabel('R¬≤ Score', fontsize=12)\n",
    "        ax4.set_title('Full Model Training History', fontsize=14, fontweight='bold')\n",
    "        ax4.legend(loc='upper left')\n",
    "        ax4_twin.legend(loc='upper right')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ablation_study_results.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"‚úÖ Visualization saved as 'ablation_study_results.png'\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY TABLE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã ABLATION STUDY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_data = []\n",
    "for name, result in successful_results.items():\n",
    "    config = ABLATION_CONFIGS[name]\n",
    "    summary_data.append({\n",
    "        'Configuration': name,\n",
    "        'Description': config['description'],\n",
    "        'R¬≤': result['r2'],\n",
    "        'MAE': result['mae'],\n",
    "        'RMSE': result['rmse'],\n",
    "        'R¬≤_Drop_%': (successful_results['FULL_MODEL']['r2'] - result['r2']) * 100\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('R¬≤', ascending=False)\n",
    "summary_df.to_csv('ablation_study_summary.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ KEY FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find most impactful components\n",
    "if len(successful_results) > 1:\n",
    "    full_r2 = successful_results['FULL_MODEL']['r2']\n",
    "    \n",
    "    impact = {}\n",
    "    for name, result in successful_results.items():\n",
    "        if name != 'FULL_MODEL':\n",
    "            impact[name] = full_r2 - result['r2']\n",
    "    \n",
    "    sorted_impact = sorted(impact.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\nüîç Most Impactful Components (by R¬≤ drop):\")\n",
    "    for i, (name, drop) in enumerate(sorted_impact[:5], 1):\n",
    "        percentage = (drop / full_r2) * 100 if full_r2 > 0 else 0\n",
    "        print(f\"   {i}. {name}: -{drop:.4f} ({percentage:.1f}% degradation)\")\n",
    "        print(f\"      ‚Üí {ABLATION_CONFIGS[name]['description']}\")\n",
    "    \n",
    "    print(\"\\nüí° Least Impactful (most redundant):\")\n",
    "    for i, (name, drop) in enumerate(sorted_impact[-3:], 1):\n",
    "        percentage = (drop / full_r2) * 100 if full_r2 > 0 else 0\n",
    "        print(f\"   {i}. {name}: -{drop:.4f} ({percentage:.1f}% degradation)\")\n",
    "        print(f\"      ‚Üí {ABLATION_CONFIGS[name]['description']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Ablation study complete!\")\n",
    "print(f\"   Tested {len(successful_results)} configurations\")\n",
    "print(f\"   Results saved to 'ablation_study_summary.csv'\")\n",
    "print(f\"   Visualizations saved to 'ablation_study_results.png'\")\n",
    "\n",
    "# ============================================================================\n",
    "# COMPONENT CONTRIBUTION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üßÆ COMPONENT CONTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'FULL_MODEL' in successful_results:\n",
    "    full_r2 = successful_results['FULL_MODEL']['r2']\n",
    "    full_mae = successful_results['FULL_MODEL']['mae']\n",
    "    \n",
    "    contributions = {\n",
    "        'Images': {\n",
    "            'r2_contribution': full_r2 - successful_results.get('NO_IMAGES', {}).get('r2', 0) if 'NO_IMAGES' in successful_results else 0,\n",
    "            'mae_improvement': successful_results.get('NO_IMAGES', {}).get('mae', 0) - full_mae if 'NO_IMAGES' in successful_results else 0\n",
    "        },\n",
    "        'Tabular Features': {\n",
    "            'r2_contribution': full_r2 - successful_results.get('NO_TABULAR', {}).get('r2', 0) if 'NO_TABULAR' in successful_results else 0,\n",
    "            'mae_improvement': successful_results.get('NO_TABULAR', {}).get('mae', 0) - full_mae if 'NO_TABULAR' in successful_results else 0\n",
    "        },\n",
    "        'Data Augmentation': {\n",
    "            'r2_contribution': full_r2 - successful_results.get('NO_AUGMENTATION', {}).get('r2', 0) if 'NO_AUGMENTATION' in successful_results else 0,\n",
    "            'mae_improvement': successful_results.get('NO_AUGMENTATION', {}).get('mae', 0) - full_mae if 'NO_AUGMENTATION' in successful_results else 0\n",
    "        },\n",
    "        'Derived Features': {\n",
    "            'r2_contribution': full_r2 - successful_results.get('NO_DERIVED_FEATURES', {}).get('r2', 0) if 'NO_DERIVED_FEATURES' in successful_results else 0,\n",
    "            'mae_improvement': successful_results.get('NO_DERIVED_FEATURES', {}).get('mae', 0) - full_mae if 'NO_DERIVED_FEATURES' in successful_results else 0\n",
    "        },\n",
    "        'Dropout': {\n",
    "            'r2_contribution': full_r2 - successful_results.get('NO_DROPOUT', {}).get('r2', 0) if 'NO_DROPOUT' in successful_results else 0,\n",
    "            'mae_improvement': successful_results.get('NO_DROPOUT', {}).get('mae', 0) - full_mae if 'NO_DROPOUT' in successful_results else 0\n",
    "        },\n",
    "        'Batch Normalization': {\n",
    "            'r2_contribution': full_r2 - successful_results.get('NO_BATCH_NORM', {}).get('r2', 0) if 'NO_BATCH_NORM' in successful_results else 0,\n",
    "            'mae_improvement': successful_results.get('NO_BATCH_NORM', {}).get('mae', 0) - full_mae if 'NO_BATCH_NORM' in successful_results else 0\n",
    "        },\n",
    "        'Pretrained Weights': {\n",
    "            'r2_contribution': full_r2 - successful_results.get('NO_PRETRAINED', {}).get('r2', 0) if 'NO_PRETRAINED' in successful_results else 0,\n",
    "            'mae_improvement': successful_results.get('NO_PRETRAINED', {}).get('mae', 0) - full_mae if 'NO_PRETRAINED' in successful_results else 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüìä Individual Component Contributions:\")\n",
    "    contrib_df = pd.DataFrame(contributions).T\n",
    "    contrib_df = contrib_df.sort_values('r2_contribution', ascending=False)\n",
    "    \n",
    "    for component, values in contrib_df.iterrows():\n",
    "        r2_contrib = values['r2_contribution']\n",
    "        mae_improve = values['mae_improvement']\n",
    "        percentage = (r2_contrib / full_r2) * 100 if full_r2 > 0 else 0\n",
    "        \n",
    "        print(f\"\\n   {component}:\")\n",
    "        print(f\"      R¬≤ Contribution: {r2_contrib:+.4f} ({percentage:+.1f}%)\")\n",
    "        print(f\"      MAE Improvement: {mae_improve:+.2f} kg/m¬≤\")\n",
    "        \n",
    "        # Rating\n",
    "        if abs(r2_contrib) > 0.05:\n",
    "            rating = \"üî¥ CRITICAL\"\n",
    "        elif abs(r2_contrib) > 0.02:\n",
    "            rating = \"üü° IMPORTANT\"\n",
    "        else:\n",
    "            rating = \"üü¢ MINOR\"\n",
    "        print(f\"      Impact: {rating}\")\n",
    "    \n",
    "    contrib_df.to_csv('component_contributions.csv')\n",
    "    print(\"\\n‚úÖ Component contributions saved to 'component_contributions.csv'\")\n",
    "\n",
    "# ============================================================================\n",
    "# DROPOUT RATE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéõÔ∏è DROPOUT RATE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "dropout_configs = ['NO_DROPOUT', 'FULL_MODEL', 'HIGH_DROPOUT']\n",
    "dropout_available = [c for c in dropout_configs if c in successful_results]\n",
    "\n",
    "if len(dropout_available) > 1:\n",
    "    print(\"\\nüìä Dropout Rate vs Performance:\")\n",
    "    dropout_rates = {\n",
    "        'NO_DROPOUT': 0.0,\n",
    "        'FULL_MODEL': 0.3,\n",
    "        'HIGH_DROPOUT': 0.5\n",
    "    }\n",
    "    \n",
    "    for config in dropout_available:\n",
    "        rate = dropout_rates[config]\n",
    "        result = successful_results[config]\n",
    "        print(f\"   Dropout {rate:.1f}: R¬≤={result['r2']:.4f}, MAE={result['mae']:.2f}\")\n",
    "    \n",
    "    # Find optimal\n",
    "    best_dropout_config = max(dropout_available, \n",
    "                             key=lambda x: successful_results[x]['r2'])\n",
    "    optimal_rate = dropout_rates[best_dropout_config]\n",
    "    print(f\"\\nüéØ Optimal Dropout Rate: {optimal_rate:.1f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# MODALITY COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîÄ MODALITY COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "modality_configs = {\n",
    "    'Images Only': 'NO_TABULAR',\n",
    "    'Tabular Only': 'NO_IMAGES',\n",
    "    'Multimodal (Both)': 'FULL_MODEL'\n",
    "}\n",
    "\n",
    "available_modalities = {k: v for k, v in modality_configs.items() \n",
    "                        if v in successful_results}\n",
    "\n",
    "if len(available_modalities) >= 2:\n",
    "    print(\"\\nüìä Performance by Modality:\")\n",
    "    \n",
    "    modality_results = []\n",
    "    for name, config in available_modalities.items():\n",
    "        result = successful_results[config]\n",
    "        modality_results.append({\n",
    "            'Modality': name,\n",
    "            'R¬≤': result['r2'],\n",
    "            'MAE': result['mae'],\n",
    "            'RMSE': result['rmse']\n",
    "        })\n",
    "        print(f\"   {name}:\")\n",
    "        print(f\"      R¬≤: {result['r2']:.4f}\")\n",
    "        print(f\"      MAE: {result['mae']:.2f} kg/m¬≤\")\n",
    "    \n",
    "    # Calculate synergy\n",
    "    if 'FULL_MODEL' in successful_results and 'NO_IMAGES' in successful_results and 'NO_TABULAR' in successful_results:\n",
    "        full_r2 = successful_results['FULL_MODEL']['r2']\n",
    "        images_only_r2 = successful_results['NO_TABULAR']['r2']\n",
    "        tabular_only_r2 = successful_results['NO_IMAGES']['r2']\n",
    "        \n",
    "        expected_combined = (images_only_r2 + tabular_only_r2) / 2\n",
    "        actual_combined = full_r2\n",
    "        synergy = actual_combined - expected_combined\n",
    "        \n",
    "        print(f\"\\nüî¨ Multimodal Synergy Analysis:\")\n",
    "        print(f\"   Images Only R¬≤: {images_only_r2:.4f}\")\n",
    "        print(f\"   Tabular Only R¬≤: {tabular_only_r2:.4f}\")\n",
    "        print(f\"   Expected Combined (avg): {expected_combined:.4f}\")\n",
    "        print(f\"   Actual Multimodal R¬≤: {actual_combined:.4f}\")\n",
    "        print(f\"   Synergy Effect: {synergy:+.4f}\")\n",
    "        \n",
    "        if synergy > 0.01:\n",
    "            print(f\"   ‚Üí ‚ú® POSITIVE SYNERGY: Modalities complement each other!\")\n",
    "        elif synergy < -0.01:\n",
    "            print(f\"   ‚Üí ‚ö†Ô∏è NEGATIVE SYNERGY: Potential interference between modalities\")\n",
    "        else:\n",
    "            print(f\"   ‚Üí ‚ûñ NEUTRAL: Modalities are independent\")\n",
    "\n",
    "# ============================================================================\n",
    "# REGULARIZATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üõ°Ô∏è REGULARIZATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "reg_configs = {\n",
    "    'No Regularization': 'MINIMAL_MODEL',\n",
    "    'Dropout Only': None,\n",
    "    'Batch Norm Only': None,\n",
    "    'Full Regularization': 'FULL_MODEL'\n",
    "}\n",
    "\n",
    "print(\"\\nüìä Regularization Impact:\")\n",
    "if 'MINIMAL_MODEL' in successful_results and 'FULL_MODEL' in successful_results:\n",
    "    minimal_r2 = successful_results['MINIMAL_MODEL']['r2']\n",
    "    full_r2 = successful_results['FULL_MODEL']['r2']\n",
    "    \n",
    "    improvement = full_r2 - minimal_r2\n",
    "    percentage = (improvement / minimal_r2) * 100 if minimal_r2 > 0 else 0\n",
    "    \n",
    "    print(f\"   Minimal Model R¬≤: {minimal_r2:.4f}\")\n",
    "    print(f\"   Full Regularization R¬≤: {full_r2:.4f}\")\n",
    "    print(f\"   Improvement: {improvement:+.4f} ({percentage:+.1f}%)\")\n",
    "    \n",
    "    if improvement > 0.02:\n",
    "        print(f\"   ‚Üí ‚úÖ Regularization provides significant benefit\")\n",
    "    else:\n",
    "        print(f\"   ‚Üí ‚ö†Ô∏è Regularization has minimal impact\")\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING EFFICIENCY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚ö° TRAINING EFFICIENCY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä Epochs to Convergence:\")\n",
    "for name, result in successful_results.items():\n",
    "    if result and 'history' in result:\n",
    "        epochs_trained = len(result['history']['train_loss'])\n",
    "        final_r2 = result['r2']\n",
    "        \n",
    "        # Find epoch where 95% of final R¬≤ was reached\n",
    "        if 'val_r2' in result['history']:\n",
    "            val_r2_history = result['history']['val_r2']\n",
    "            target_r2 = final_r2 * 0.95\n",
    "            \n",
    "            epochs_to_95 = epochs_trained\n",
    "            for i, r2 in enumerate(val_r2_history):\n",
    "                if r2 >= target_r2:\n",
    "                    epochs_to_95 = i + 1\n",
    "                    break\n",
    "            \n",
    "            print(f\"   {name}:\")\n",
    "            print(f\"      Total epochs: {epochs_trained}\")\n",
    "            print(f\"      Epochs to 95% performance: {epochs_to_95}\")\n",
    "            efficiency = (epochs_to_95/epochs_trained)*100 if epochs_trained > 0 else 0\n",
    "            print(f\"      Efficiency: {efficiency:.1f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# RECOMMENDATIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üí° RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(successful_results) > 0:\n",
    "    full_r2 = successful_results.get('FULL_MODEL', {}).get('r2', 0)\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # Check each component\n",
    "    if 'NO_IMAGES' in successful_results:\n",
    "        images_drop = full_r2 - successful_results['NO_IMAGES']['r2']\n",
    "        if images_drop > 0.05:\n",
    "            recommendations.append(\"‚úÖ KEEP: Images are critical (R¬≤ drop: {:.3f})\".format(images_drop))\n",
    "        elif images_drop < 0.01:\n",
    "            recommendations.append(\"‚ùå CONSIDER REMOVING: Images add minimal value (R¬≤ drop: {:.3f})\".format(images_drop))\n",
    "    \n",
    "    if 'NO_TABULAR' in successful_results:\n",
    "        tabular_drop = full_r2 - successful_results['NO_TABULAR']['r2']\n",
    "        if tabular_drop > 0.05:\n",
    "            recommendations.append(\"‚úÖ KEEP: Tabular features are critical (R¬≤ drop: {:.3f})\".format(tabular_drop))\n",
    "        elif tabular_drop < 0.01:\n",
    "            recommendations.append(\"‚ùå CONSIDER REMOVING: Tabular features add minimal value (R¬≤ drop: {:.3f})\".format(tabular_drop))\n",
    "    \n",
    "    if 'NO_AUGMENTATION' in successful_results:\n",
    "        aug_drop = full_r2 - successful_results['NO_AUGMENTATION']['r2']\n",
    "        if aug_drop > 0.02:\n",
    "            recommendations.append(\"‚úÖ KEEP: Data augmentation is important (R¬≤ drop: {:.3f})\".format(aug_drop))\n",
    "        elif aug_drop < 0.005:\n",
    "            recommendations.append(\"‚ö†Ô∏è OPTIONAL: Data augmentation has minimal impact (R¬≤ drop: {:.3f})\".format(aug_drop))\n",
    "    \n",
    "    if 'NO_DERIVED_FEATURES' in successful_results:\n",
    "        derived_drop = full_r2 - successful_results['NO_DERIVED_FEATURES']['r2']\n",
    "        if derived_drop > 0.02:\n",
    "            recommendations.append(\"‚úÖ KEEP: Derived features are valuable (R¬≤ drop: {:.3f})\".format(derived_drop))\n",
    "        elif derived_drop < 0.005:\n",
    "            recommendations.append(\"‚ö†Ô∏è SIMPLIFY: Derived features add minimal value (R¬≤ drop: {:.3f})\".format(derived_drop))\n",
    "    \n",
    "    if 'NO_DROPOUT' in successful_results:\n",
    "        dropout_drop = full_r2 - successful_results['NO_DROPOUT']['r2']\n",
    "        if dropout_drop > 0.01:\n",
    "            recommendations.append(\"‚úÖ KEEP: Dropout prevents overfitting (R¬≤ drop: {:.3f})\".format(dropout_drop))\n",
    "    \n",
    "    if 'NO_PRETRAINED' in successful_results:\n",
    "        pretrain_drop = full_r2 - successful_results['NO_PRETRAINED']['r2']\n",
    "        if pretrain_drop > 0.03:\n",
    "            recommendations.append(\"‚úÖ KEEP: Pretrained weights are crucial (R¬≤ drop: {:.3f})\".format(pretrain_drop))\n",
    "        elif pretrain_drop < 0.01:\n",
    "            recommendations.append(\"‚ö†Ô∏è OPTIONAL: Pretrained weights have minimal impact (R¬≤ drop: {:.3f})\".format(pretrain_drop))\n",
    "    \n",
    "    print(\"\\nüéØ Based on the ablation study:\")\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"   {i}. {rec}\")\n",
    "    \n",
    "    # Overall strategy\n",
    "    print(\"\\nüìã Suggested Model Strategy:\")\n",
    "    \n",
    "    if len(recommendations) > 0:\n",
    "        # Determine best configuration\n",
    "        critical_components = [r for r in recommendations if '‚úÖ KEEP' in r and 'critical' in r.lower()]\n",
    "        \n",
    "        if len(critical_components) >= 2:\n",
    "            print(\"   ‚Üí Use FULL MULTIMODAL approach\")\n",
    "            print(\"   ‚Üí Both modalities provide significant value\")\n",
    "        elif 'Images are critical' in str(recommendations):\n",
    "            print(\"   ‚Üí Focus on IMAGE-BASED approach with light tabular features\")\n",
    "        elif 'Tabular features are critical' in str(recommendations):\n",
    "            print(\"   ‚Üí Focus on TABULAR approach with light image features\")\n",
    "        else:\n",
    "            print(\"   ‚Üí Use BALANCED approach with moderate complexity\")\n",
    "    \n",
    "    # Complexity vs Performance tradeoff\n",
    "    print(\"\\n‚öñÔ∏è Complexity vs Performance Tradeoff:\")\n",
    "    if 'MINIMAL_MODEL' in successful_results and 'FULL_MODEL' in successful_results:\n",
    "        minimal = successful_results['MINIMAL_MODEL']\n",
    "        full = successful_results['FULL_MODEL']\n",
    "        \n",
    "        complexity_cost = \"High\"\n",
    "        performance_gain = full['r2'] - minimal['r2']\n",
    "        \n",
    "        print(f\"   Minimal Model: R¬≤={minimal['r2']:.4f} (Low complexity)\")\n",
    "        print(f\"   Full Model: R¬≤={full['r2']:.4f} (High complexity)\")\n",
    "        print(f\"   Performance Gain: {performance_gain:+.4f}\")\n",
    "        \n",
    "        if performance_gain > 0.05:\n",
    "            print(\"   ‚Üí ‚úÖ Additional complexity is JUSTIFIED\")\n",
    "        elif performance_gain > 0.02:\n",
    "            print(\"   ‚Üí ‚ö†Ô∏è Additional complexity shows MODERATE benefit\")\n",
    "        else:\n",
    "            print(\"   ‚Üí ‚ùå Additional complexity NOT justified - use simpler model\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXPORT DETAILED RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üíæ EXPORTING DETAILED RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comprehensive report\n",
    "report_data = []\n",
    "for name, result in successful_results.items():\n",
    "    config = ABLATION_CONFIGS[name]\n",
    "    \n",
    "    report_entry = {\n",
    "        'Configuration': name,\n",
    "        'Description': config['description'],\n",
    "        'Use_Images': config['use_images'],\n",
    "        'Use_Tabular': config['use_tabular'],\n",
    "        'Augmentation': config['use_augmentation'],\n",
    "        'Derived_Features': config['use_derived_features'],\n",
    "        'Dropout': config['dropout_rate'],\n",
    "        'Batch_Norm': config['use_batch_norm'],\n",
    "        'Pretrained': config['use_pretrained'],\n",
    "        'R¬≤': result['r2'],\n",
    "        'MAE': result['mae'],\n",
    "        'RMSE': result['rmse'],\n",
    "        'Epochs_Trained': len(result['history']['train_loss'])\n",
    "    }\n",
    "    \n",
    "    # Add relative performance\n",
    "    if 'FULL_MODEL' in successful_results:\n",
    "        full_r2 = successful_results['FULL_MODEL']['r2']\n",
    "        report_entry['R¬≤_vs_Full'] = result['r2'] - full_r2\n",
    "        report_entry['R¬≤_vs_Full_%'] = ((result['r2'] - full_r2) / full_r2) * 100 if full_r2 > 0 else 0\n",
    "    \n",
    "    report_data.append(report_entry)\n",
    "\n",
    "detailed_report_df = pd.DataFrame(report_data)\n",
    "detailed_report_df = detailed_report_df.sort_values('R¬≤', ascending=False)\n",
    "detailed_report_df.to_csv('ablation_detailed_report.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Detailed report saved to 'ablation_detailed_report.csv'\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéì ABLATION STUDY COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Summary Statistics:\")\n",
    "print(f\"   Configurations tested: {len(successful_results)}/{len(ABLATION_CONFIGS)}\")\n",
    "if len(successful_results) > 0:\n",
    "    print(f\"   Best R¬≤: {max([r['r2'] for r in successful_results.values()]):.4f}\")\n",
    "    print(f\"   Worst R¬≤: {min([r['r2'] for r in successful_results.values()]):.4f}\")\n",
    "    print(f\"   R¬≤ Range: {max([r['r2'] for r in successful_results.values()]) - min([r['r2'] for r in successful_results.values()]):.4f}\")\n",
    "\n",
    "if 'FULL_MODEL' in successful_results:\n",
    "    full_model = successful_results['FULL_MODEL']\n",
    "    print(f\"\\nüèÜ Full Model Performance:\")\n",
    "    print(f\"   R¬≤: {full_model['r2']:.4f}\")\n",
    "    print(f\"   MAE: {full_model['mae']:.2f} kg/m¬≤\")\n",
    "    print(f\"   RMSE: {full_model['rmse']:.2f} kg/m¬≤\")\n",
    "\n",
    "print(f\"\\nüìÅ Generated Files:\")\n",
    "print(f\"   ‚Ä¢ ablation_study_summary.csv\")\n",
    "print(f\"   ‚Ä¢ ablation_detailed_report.csv\")\n",
    "print(f\"   ‚Ä¢ component_contributions.csv\")\n",
    "print(f\"   ‚Ä¢ ablation_study_results.png\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Total Time: {total_time/60:.1f} minutes\")\n",
    "\n",
    "if FAST_MODE:\n",
    "    print(f\"\\n‚ö° FAST MODE was enabled:\")\n",
    "    print(f\"   ‚Ä¢ Used {SAMPLE_SIZE} samples instead of full dataset\")\n",
    "    print(f\"   ‚Ä¢ Trained for {NUM_EPOCHS} epochs instead of 15\")\n",
    "    print(f\"   ‚Ä¢ Set FAST_MODE=False and re-run for full training\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚ú® Thank you for using the Fast Ablation Study Framework!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T16:40:52.654201Z",
     "iopub.status.busy": "2026-01-11T16:40:52.653901Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "from scipy import stats\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# FAST ABLATION CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "FAST_MODE = True\n",
    "SAMPLE_SIZE = 5000\n",
    "NUM_EPOCHS = 30\n",
    "PATIENCE = 3\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"‚ö° ENHANCED ABLATION STUDY WITH COMPREHENSIVE METRICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üìä Sample Size: {SAMPLE_SIZE}\")\n",
    "print(f\"üîÑ Max Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"‚è±Ô∏è  Patience: {PATIENCE}\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# ABLATION STUDY CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "ABLATION_CONFIGS = {\n",
    "    'FULL_MODEL': {\n",
    "        'use_images': True,\n",
    "        'use_tabular': True,\n",
    "        'use_augmentation': True,\n",
    "        'use_derived_features': True,\n",
    "        'use_dropout': True,\n",
    "        'dropout_rate': 0.3,\n",
    "        'use_batch_norm': True,\n",
    "        'use_pretrained': True,\n",
    "        'augment_multiplier': 3,\n",
    "        'feature_noise': 0.02,\n",
    "        'description': 'Full multimodal model with all features'\n",
    "    },\n",
    "    'NO_IMAGES': {\n",
    "        'use_images': False,\n",
    "        'use_tabular': True,\n",
    "        'use_augmentation': True,\n",
    "        'use_derived_features': True,\n",
    "        'use_dropout': True,\n",
    "        'dropout_rate': 0.3,\n",
    "        'use_batch_norm': True,\n",
    "        'use_pretrained': True,\n",
    "        'augment_multiplier': 3,\n",
    "        'feature_noise': 0.02,\n",
    "        'description': 'Tabular features only (no images)'\n",
    "    },\n",
    "    'NO_TABULAR': {\n",
    "        'use_images': True,\n",
    "        'use_tabular': False,\n",
    "        'use_augmentation': True,\n",
    "        'use_derived_features': False,\n",
    "        'use_dropout': True,\n",
    "        'dropout_rate': 0.3,\n",
    "        'use_batch_norm': True,\n",
    "        'use_pretrained': True,\n",
    "        'augment_multiplier': 3,\n",
    "        'feature_noise': 0.0,\n",
    "        'description': 'Images only (no tabular features)'\n",
    "    },\n",
    "    'NO_AUGMENTATION': {\n",
    "        'use_images': True,\n",
    "        'use_tabular': True,\n",
    "        'use_augmentation': False,\n",
    "        'use_derived_features': True,\n",
    "        'use_dropout': True,\n",
    "        'dropout_rate': 0.3,\n",
    "        'use_batch_norm': True,\n",
    "        'use_pretrained': True,\n",
    "        'augment_multiplier': 3,\n",
    "        'feature_noise': 0.0,\n",
    "        'description': 'No data augmentation'\n",
    "    },\n",
    "    'NO_DERIVED_FEATURES': {\n",
    "        'use_images': True,\n",
    "        'use_tabular': True,\n",
    "        'use_augmentation': True,\n",
    "        'use_derived_features': False,\n",
    "        'use_dropout': True,\n",
    "        'dropout_rate': 0.3,\n",
    "        'use_batch_norm': True,\n",
    "        'use_pretrained': True,\n",
    "        'augment_multiplier': 2,\n",
    "        'feature_noise': 0.02,\n",
    "        'description': 'No engineered/derived features'\n",
    "    },\n",
    "    'NO_DROPOUT': {\n",
    "        'use_images': True,\n",
    "        'use_tabular': True,\n",
    "        'use_augmentation': True,\n",
    "        'use_derived_features': True,\n",
    "        'use_dropout': False,\n",
    "        'dropout_rate': 0.0,\n",
    "        'use_batch_norm': True,\n",
    "        'use_pretrained': True,\n",
    "        'augment_multiplier': 3,\n",
    "        'feature_noise': 0.02,\n",
    "        'description': 'No dropout regularization'\n",
    "    },\n",
    "    'HIGH_DROPOUT': {\n",
    "        'use_images': True,\n",
    "        'use_tabular': True,\n",
    "        'use_augmentation': True,\n",
    "        'use_derived_features': True,\n",
    "        'use_dropout': True,\n",
    "        'dropout_rate': 0.5,\n",
    "        'use_batch_norm': True,\n",
    "        'use_pretrained': True,\n",
    "        'augment_multiplier': 3,\n",
    "        'feature_noise': 0.02,\n",
    "        'description': 'High dropout (0.5 instead of 0.3)'\n",
    "    },\n",
    "    'NO_BATCH_NORM': {\n",
    "        'use_images': True,\n",
    "        'use_tabular': True,\n",
    "        'use_augmentation': True,\n",
    "        'use_derived_features': True,\n",
    "        'use_dropout': True,\n",
    "        'dropout_rate': 0.3,\n",
    "        'use_batch_norm': False,\n",
    "        'use_pretrained': True,\n",
    "        'augment_multiplier': 3,\n",
    "        'feature_noise': 0.02,\n",
    "        'description': 'No batch normalization'\n",
    "    },\n",
    "    'NO_PRETRAINED': {\n",
    "        'use_images': True,\n",
    "        'use_tabular': True,\n",
    "        'use_augmentation': True,\n",
    "        'use_derived_features': True,\n",
    "        'use_dropout': True,\n",
    "        'dropout_rate': 0.3,\n",
    "        'use_batch_norm': True,\n",
    "        'use_pretrained': False,\n",
    "        'augment_multiplier': 3,\n",
    "        'feature_noise': 0.02,\n",
    "        'description': 'Random initialization (no pretrained weights)'\n",
    "    },\n",
    "    'MINIMAL_MODEL': {\n",
    "        'use_images': True,\n",
    "        'use_tabular': True,\n",
    "        'use_augmentation': False,\n",
    "        'use_derived_features': False,\n",
    "        'use_dropout': False,\n",
    "        'dropout_rate': 0.0,\n",
    "        'use_batch_norm': False,\n",
    "        'use_pretrained': False,\n",
    "        'augment_multiplier': 3,\n",
    "        'feature_noise': 0.0,\n",
    "        'description': 'Minimal model (basic features, no regularization)'\n",
    "    }\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP\n",
    "# ============================================================================\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = '/kaggle/input/bmi-dataset'\n",
    "TRAIN_CSV = '/kaggle/input/bmi-dataset/train.csv'\n",
    "TEST_CSV = '/kaggle/input/bmi-dataset/test.csv'\n",
    "IMAGE_DIR = '/kaggle/input/bmi-dataset/ROI'\n",
    "\n",
    "print(\"üî¨ ENHANCED ABLATION STUDY - BMI PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    train_df = pd.read_csv(TRAIN_CSV)\n",
    "    test_df = pd.read_csv(TEST_CSV)\n",
    "    print(f\"‚úÖ Data loaded: Train={train_df.shape}, Test={test_df.shape}\")\n",
    "except:\n",
    "    print(\"‚ùå Error loading data. Using synthetic data for demonstration.\")\n",
    "    n_train, n_test = 1000, 200\n",
    "    train_df = pd.DataFrame({\n",
    "        'BMI': np.random.normal(25, 5, n_train),\n",
    "        'age': np.random.randint(18, 80, n_train),\n",
    "        'sex_encoded': np.random.randint(0, 2, n_train),\n",
    "        'face_height': np.random.normal(100, 10, n_train),\n",
    "        'face_width_cheeks': np.random.normal(80, 8, n_train),\n",
    "        'face_width_jaw': np.random.normal(75, 8, n_train),\n",
    "        'left_eye_width': np.random.normal(20, 2, n_train),\n",
    "        'right_eye_width': np.random.normal(20, 2, n_train),\n",
    "        'nose_length': np.random.normal(30, 3, n_train),\n",
    "        'nose_width': np.random.normal(25, 2.5, n_train),\n",
    "        'image_filename': [f'img_{i}.jpg' for i in range(n_train)]\n",
    "    })\n",
    "    test_df = train_df.copy().sample(n_test)\n",
    "    IMAGE_DIR = None\n",
    "\n",
    "# Sample data\n",
    "if FAST_MODE and len(train_df) > SAMPLE_SIZE:\n",
    "    print(f\"\\n‚ö° Fast Mode: Sampling {SAMPLE_SIZE} rows from {len(train_df)}\")\n",
    "    train_df = train_df.sample(SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n",
    "    print(f\"‚úÖ Using {len(train_df)} samples for ablation study\")\n",
    "\n",
    "# Drop height/weight\n",
    "for col in ['height', 'weight']:\n",
    "    if col in train_df.columns:\n",
    "        train_df = train_df.drop(columns=[col])\n",
    "        test_df = test_df.drop(columns=[col])\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "def prepare_features(df, include_derived=True):\n",
    "    df_eng = df.copy()\n",
    "    basic_features = ['face_height', 'face_width_cheeks', 'face_width_jaw',\n",
    "                      'left_eye_width', 'right_eye_width', 'nose_length', \n",
    "                      'nose_width', 'age', 'sex_encoded']\n",
    "    available_features = [f for f in basic_features if f in df_eng.columns]\n",
    "    \n",
    "    if include_derived:\n",
    "        if 'face_height' in df_eng.columns and 'face_width_cheeks' in df_eng.columns:\n",
    "            df_eng['fwhr'] = (df_eng['face_width_cheeks'] / \n",
    "                             df_eng['face_height'].clip(lower=1e-8)).fillna(0)\n",
    "            available_features.append('fwhr')\n",
    "        \n",
    "        if 'left_eye_width' in df_eng.columns and 'right_eye_width' in df_eng.columns:\n",
    "            sum_eyes = df_eng['left_eye_width'] + df_eng['right_eye_width']\n",
    "            df_eng['eye_symmetry'] = 1 - abs(df_eng['left_eye_width'] - \n",
    "                                             df_eng['right_eye_width']) / sum_eyes.clip(lower=1e-8)\n",
    "            df_eng['eye_symmetry'] = df_eng['eye_symmetry'].fillna(0)\n",
    "            available_features.append('eye_symmetry')\n",
    "        \n",
    "        if 'face_width_cheeks' in df_eng.columns and 'face_height' in df_eng.columns:\n",
    "            df_eng['face_area'] = (df_eng['face_width_cheeks'] * \n",
    "                                  df_eng['face_height']).fillna(0)\n",
    "            available_features.append('face_area')\n",
    "    \n",
    "    return df_eng, available_features\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class AblationDataset(Dataset):\n",
    "    def __init__(self, df, feature_cols, image_dir, config, is_training=True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.feature_cols = feature_cols\n",
    "        self.image_dir = image_dir\n",
    "        self.config = config\n",
    "        self.is_training = is_training\n",
    "        \n",
    "        if config['use_tabular']:\n",
    "            self.features = df[feature_cols].values.astype(np.float32)\n",
    "        self.targets = df['BMI'].values.astype(np.float32)\n",
    "        \n",
    "        if 'image_filename' in df.columns:\n",
    "            self.filenames = df['image_filename'].tolist()\n",
    "        else:\n",
    "            self.filenames = [f'img_{i}.jpg' for i in range(len(df))]\n",
    "        \n",
    "        if config['use_images']:\n",
    "            if is_training and config['use_augmentation']:\n",
    "                self.transform = transforms.Compose([\n",
    "                    transforms.Resize((256, 256)),\n",
    "                    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "                    transforms.RandomHorizontalFlip(p=0.5),\n",
    "                    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                       std=[0.229, 0.224, 0.225])\n",
    "                ])\n",
    "            else:\n",
    "                self.transform = transforms.Compose([\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                       std=[0.229, 0.224, 0.225])\n",
    "                ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.is_training and self.config['augment_multiplier'] > 1:\n",
    "            return len(self.df) * self.config['augment_multiplier']\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        original_idx = idx % len(self.df)\n",
    "        is_augmented = idx >= len(self.df)\n",
    "        target = torch.tensor(self.targets[original_idx], dtype=torch.float32)\n",
    "        \n",
    "        if self.config['use_tabular']:\n",
    "            features = torch.tensor(self.features[original_idx], dtype=torch.float32)\n",
    "            if self.is_training and is_augmented and self.config['feature_noise'] > 0:\n",
    "                noise = torch.randn_like(features) * self.config['feature_noise']\n",
    "                features = features + noise\n",
    "        else:\n",
    "            features = torch.zeros(1)\n",
    "        \n",
    "        if self.config['use_images'] and self.image_dir is not None:\n",
    "            filename = self.filenames[original_idx]\n",
    "            img_path = os.path.join(self.image_dir, filename)\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    if img.mode != 'RGB':\n",
    "                        img = img.convert('RGB')\n",
    "                    image = self.transform(img)\n",
    "            except:\n",
    "                image = torch.zeros(3, 224, 224)\n",
    "        else:\n",
    "            image = torch.randn(3, 224, 224) * 0.1\n",
    "        \n",
    "        return {'features': features, 'image': image, 'target': target}\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL ARCHITECTURE\n",
    "# ============================================================================\n",
    "\n",
    "class AblationModel(nn.Module):\n",
    "    def __init__(self, num_features, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        if config['use_images']:\n",
    "            if config['use_pretrained']:\n",
    "                resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "            else:\n",
    "                resnet = models.resnet18(weights=None)\n",
    "            \n",
    "            for param in resnet.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.image_features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "            image_dim = 512\n",
    "        else:\n",
    "            image_dim = 0\n",
    "        \n",
    "        if config['use_tabular']:\n",
    "            layers = []\n",
    "            layers.append(nn.Linear(num_features, 128))\n",
    "            if config['use_batch_norm']:\n",
    "                layers.append(nn.BatchNorm1d(128))\n",
    "            layers.append(nn.ReLU())\n",
    "            if config['use_dropout']:\n",
    "                layers.append(nn.Dropout(config['dropout_rate']))\n",
    "            \n",
    "            layers.append(nn.Linear(128, 64))\n",
    "            if config['use_batch_norm']:\n",
    "                layers.append(nn.BatchNorm1d(64))\n",
    "            layers.append(nn.ReLU())\n",
    "            \n",
    "            self.tabular_features = nn.Sequential(*layers)\n",
    "            tabular_dim = 64\n",
    "        else:\n",
    "            tabular_dim = 0\n",
    "        \n",
    "        combined_dim = image_dim + tabular_dim\n",
    "        \n",
    "        if combined_dim == 0:\n",
    "            raise ValueError(\"Must use at least images or tabular features!\")\n",
    "        \n",
    "        fusion_layers = []\n",
    "        fusion_layers.append(nn.Linear(combined_dim, 128))\n",
    "        if config['use_batch_norm']:\n",
    "            fusion_layers.append(nn.BatchNorm1d(128))\n",
    "        fusion_layers.append(nn.ReLU())\n",
    "        if config['use_dropout']:\n",
    "            fusion_layers.append(nn.Dropout(config['dropout_rate']))\n",
    "        \n",
    "        fusion_layers.append(nn.Linear(128, 64))\n",
    "        if config['use_batch_norm']:\n",
    "            fusion_layers.append(nn.BatchNorm1d(64))\n",
    "        fusion_layers.append(nn.ReLU())\n",
    "        \n",
    "        self.fusion = nn.Sequential(*fusion_layers)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, features, image):\n",
    "        batch_size = features.size(0)\n",
    "        feat_list = []\n",
    "        \n",
    "        if self.config['use_images']:\n",
    "            img_feat = self.image_features(image)\n",
    "            img_feat = img_feat.view(batch_size, -1)\n",
    "            feat_list.append(img_feat)\n",
    "        \n",
    "        if self.config['use_tabular']:\n",
    "            tab_feat = self.tabular_features(features)\n",
    "            feat_list.append(tab_feat)\n",
    "        \n",
    "        combined = torch.cat(feat_list, dim=1)\n",
    "        fused = self.fusion(combined)\n",
    "        output = self.output(fused)\n",
    "        \n",
    "        return output.squeeze()\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS FOR METRICS\n",
    "# ============================================================================\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count total and trainable parameters\"\"\"\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "def get_model_size(model):\n",
    "    \"\"\"Get model size in MB\"\"\"\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    size_mb = (param_size + buffer_size) / 1024**2\n",
    "    return size_mb\n",
    "\n",
    "def measure_inference_time(model, loader, device, num_batches=10):\n",
    "    \"\"\"Measure average inference time per sample\"\"\"\n",
    "    model.eval()\n",
    "    times = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "            \n",
    "            features = batch['features'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            batch_size = features.size(0)\n",
    "            \n",
    "            start = time.time()\n",
    "            _ = model(features, images)\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "            \n",
    "            times.append((end - start) / batch_size)\n",
    "    \n",
    "    return np.mean(times) * 1000  # Convert to ms\n",
    "\n",
    "def calculate_confidence_interval(values, confidence=0.95):\n",
    "    \"\"\"Calculate confidence interval\"\"\"\n",
    "    n = len(values)\n",
    "    mean = np.mean(values)\n",
    "    std_err = stats.sem(values)\n",
    "    margin = std_err * stats.t.ppf((1 + confidence) / 2, n - 1)\n",
    "    return mean, mean - margin, mean + margin\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING FUNCTION WITH MULTIPLE RUNS\n",
    "# ============================================================================\n",
    "\n",
    "def train_model_multiple_runs(config, train_df, val_df, feature_cols, device, \n",
    "                               num_runs=3, num_epochs=NUM_EPOCHS, patience=PATIENCE):\n",
    "    \"\"\"Train model multiple times to get statistics\"\"\"\n",
    "    \n",
    "    run_results = []\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        print(f\"   Run {run+1}/{num_runs}...\")\n",
    "        set_seed(42 + run)  # Different seed for each run\n",
    "        \n",
    "        # Prepare features\n",
    "        if config['use_derived_features']:\n",
    "            train_eng, train_features = prepare_features(train_df, include_derived=True)\n",
    "            val_eng, _ = prepare_features(val_df, include_derived=True)\n",
    "        else:\n",
    "            train_eng, train_features = prepare_features(train_df, include_derived=False)\n",
    "            val_eng, _ = prepare_features(val_df, include_derived=False)\n",
    "        \n",
    "        if config['use_tabular']:\n",
    "            use_features = [f for f in train_features if f in train_eng.columns]\n",
    "        else:\n",
    "            use_features = ['age']\n",
    "        \n",
    "        # Scale features\n",
    "        if config['use_tabular']:\n",
    "            scaler = RobustScaler()\n",
    "            train_eng[use_features] = scaler.fit_transform(\n",
    "                train_eng[use_features].fillna(0).values)\n",
    "            val_eng[use_features] = scaler.transform(\n",
    "                val_eng[use_features].fillna(0).values)\n",
    "        else:\n",
    "            scaler = None\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = AblationDataset(train_eng, use_features, IMAGE_DIR, \n",
    "                                        config, is_training=True)\n",
    "        val_dataset = AblationDataset(val_eng, use_features, IMAGE_DIR, \n",
    "                                      config, is_training=False)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, \n",
    "                                 num_workers=2)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, \n",
    "                               num_workers=2)\n",
    "        \n",
    "        # Create model\n",
    "        model = AblationModel(len(use_features) if config['use_tabular'] else 1, \n",
    "                             config).to(device)\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                               lr=1e-3, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                         factor=0.5, patience=2)\n",
    "        \n",
    "        history = {'train_loss': [], 'val_loss': [], 'val_r2': []}\n",
    "        best_val_loss = float('inf')\n",
    "        best_model_state = None\n",
    "        patience_counter = 0\n",
    "        \n",
    "        training_start = time.time()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Train\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for batch in train_loader:\n",
    "                features = batch['features'].to(device)\n",
    "                images = batch['image'].to(device)\n",
    "                targets = batch['target'].to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(features, images)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            \n",
    "            # Validate\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_preds, val_targets = [], []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    features = batch['features'].to(device)\n",
    "                    images = batch['image'].to(device)\n",
    "                    targets = batch['target'].to(device)\n",
    "                    \n",
    "                    outputs = model(features, images)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    val_preds.extend(outputs.cpu().numpy())\n",
    "                    val_targets.extend(targets.cpu().numpy())\n",
    "            \n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_r2 = r2_score(val_targets, val_preds)\n",
    "            \n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            history['train_loss'].append(avg_train_loss)\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            history['val_r2'].append(val_r2)\n",
    "            \n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_model_state = model.state_dict().copy()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "        \n",
    "        training_time = time.time() - training_start\n",
    "        \n",
    "        # Load best model\n",
    "        model.load_state_dict(best_model_state)\n",
    "        \n",
    "        # Final validation\n",
    "        model.eval()\n",
    "        val_preds, val_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                features = batch['features'].to(device)\n",
    "                images = batch['image'].to(device)\n",
    "                targets = batch['target'].to(device)\n",
    "                outputs = model(features, images)\n",
    "                val_preds.extend(outputs.cpu().numpy())\n",
    "                val_targets.extend(targets.cpu().numpy())\n",
    "        \n",
    "        val_preds = np.array(val_preds)\n",
    "        val_targets = np.array(val_targets)\n",
    "        \n",
    "        final_r2 = r2_score(val_targets, val_preds)\n",
    "        final_mae = mean_absolute_error(val_targets, val_preds)\n",
    "        final_rmse = np.sqrt(mean_squared_error(val_targets, val_preds))\n",
    "        final_mape = np.mean(np.abs((val_targets - val_preds) / val_targets)) * 100\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        residuals = val_targets - val_preds\n",
    "        mean_residual = np.mean(residuals)\n",
    "        std_residual = np.std(residuals)\n",
    "        \n",
    "        # Within threshold accuracy\n",
    "        within_1 = np.mean(np.abs(residuals) < 1) * 100\n",
    "        within_2 = np.mean(np.abs(residuals) < 2) * 100\n",
    "        within_3 = np.mean(np.abs(residuals) < 3) * 100\n",
    "        \n",
    "        # Model statistics\n",
    "        total_params, trainable_params = count_parameters(model)\n",
    "        model_size = get_model_size(model)\n",
    "        inference_time = measure_inference_time(model, val_loader, device)\n",
    "        \n",
    "        run_results.append({\n",
    "            'r2': final_r2,\n",
    "            'mae': final_mae,\n",
    "            'rmse': final_rmse,\n",
    "            'mape': final_mape,\n",
    "            'mean_residual': mean_residual,\n",
    "            'std_residual': std_residual,\n",
    "            'within_1': within_1,\n",
    "            'within_2': within_2,\n",
    "            'within_3': within_3,\n",
    "            'training_time': training_time,\n",
    "            'inference_time': inference_time,\n",
    "            'total_params': total_params,\n",
    "            'trainable_params': trainable_params,\n",
    "            'model_size': model_size,\n",
    "            'epochs_trained': len(history['train_loss']),\n",
    "            'history': history,\n",
    "            'model': model,\n",
    "            'scaler': scaler,\n",
    "            'use_features': use_features\n",
    "        })\n",
    "    \n",
    "    # Aggregate results\n",
    "    aggregated = {\n",
    "        'r2_mean': np.mean([r['r2'] for r in run_results]),\n",
    "        'r2_std': np.std([r['r2'] for r in run_results]),\n",
    "        'r2_ci': calculate_confidence_interval([r['r2'] for r in run_results]),\n",
    "        \n",
    "        'mae_mean': np.mean([r['mae'] for r in run_results]),\n",
    "        'mae_std': np.std([r['mae'] for r in run_results]),\n",
    "        'mae_ci': calculate_confidence_interval([r['mae'] for r in run_results]),\n",
    "        \n",
    "        'rmse_mean': np.mean([r['rmse'] for r in run_results]),\n",
    "        'rmse_std': np.std([r['rmse'] for r in run_results]),\n",
    "        'rmse_ci': calculate_confidence_interval([r['rmse'] for r in run_results]),\n",
    "        \n",
    "        'mape_mean': np.mean([r['mape'] for r in run_results]),\n",
    "        'mape_std': np.std([r['mape'] for r in run_results]),\n",
    "        \n",
    "        'within_1_mean': np.mean([r['within_1'] for r in run_results]),\n",
    "        'within_2_mean': np.mean([r['within_2'] for r in run_results]),\n",
    "        'within_3_mean': np.mean([r['within_3'] for r in run_results]),\n",
    "        \n",
    "        'training_time_mean': np.mean([r['training_time'] for r in run_results]),\n",
    "        'training_time_std': np.std([r['training_time'] for r in run_results]),\n",
    "        \n",
    "        'inference_time_mean': np.mean([r['inference_time'] for r in run_results]),\n",
    "        'inference_time_std': np.std([r['inference_time'] for r in run_results]),\n",
    "        \n",
    "        'total_params': run_results[0]['total_params'],\n",
    "        'trainable_params': run_results[0]['trainable_params'],\n",
    "        'model_size': run_results[0]['model_size'],\n",
    "        \n",
    "        'epochs_trained_mean': np.mean([r['epochs_trained'] for r in run_results]),\n",
    "        \n",
    "        'best_run': max(run_results, key=lambda x: x['r2']),\n",
    "        'all_runs': run_results\n",
    "    }\n",
    "    \n",
    "    return aggregated\n",
    "\n",
    "# ============================================================================\n",
    "# RUN ABLATION STUDY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ RUNNING ENHANCED ABLATION STUDY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\\n\")\n",
    "\n",
    "# Use simple train/val split\n",
    "train_idx, val_idx = train_test_split(range(len(train_df)), test_size=0.2, \n",
    "                                       random_state=42)\n",
    "train_split = train_df.iloc[train_idx].copy()\n",
    "val_split = train_df.iloc[val_idx].copy()\n",
    "\n",
    "print(f\"Train samples: {len(train_split)}, Val samples: {len(val_split)}\\n\")\n",
    "\n",
    "ablation_results = {}\n",
    "total_start = time.time()\n",
    "\n",
    "NUM_RUNS_PER_CONFIG = 3  # Number of runs for statistical significance\n",
    "\n",
    "for i, (config_name, config) in enumerate(ABLATION_CONFIGS.items(), 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üß™ [{i}/{len(ABLATION_CONFIGS)}] {config_name}\")\n",
    "    print(f\"üìù {config['description']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    config_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        result = train_model_multiple_runs(\n",
    "            config, train_split, val_split, \n",
    "            feature_cols=None,\n",
    "            device=device,\n",
    "            num_runs=NUM_RUNS_PER_CONFIG,\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            patience=PATIENCE\n",
    "        )\n",
    "        \n",
    "        config_time = time.time() - config_start\n",
    "        ablation_results[config_name] = result\n",
    "        \n",
    "        print(f\"‚úÖ Results (trained in {config_time:.1f}s):\")\n",
    "        print(f\"   R¬≤: {result['r2_mean']:.4f} ¬± {result['r2_std']:.4f}\")\n",
    "        print(f\"   MAE: {result['mae_mean']:.2f} ¬± {result['mae_std']:.2f}\")\n",
    "        print(f\"   RMSE: {result['rmse_mean']:.2f} ¬± {result['rmse_std']:.2f}\")\n",
    "        print(f\"   Inference Time: {result['inference_time_mean']:.2f}ms ¬± {result['inference_time_std']:.2f}ms\")\n",
    "        print(f\"   Total Params: {result['total_params']:,}\")\n",
    "        print(f\"   Model Size: {result['model_size']:.2f} MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        ablation_results[config_name] = None\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"\\n‚è±Ô∏è  Total training time: {total_time/60:.1f} minutes\")\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE COMPREHENSIVE ABLATION TABLE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä CREATING COMPREHENSIVE ABLATION TABLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "successful_results = {k: v for k, v in ablation_results.items() if v is not None}\n",
    "\n",
    "if len(successful_results) > 0:\n",
    "    # Prepare data for table\n",
    "    table_data = []\n",
    "    \n",
    "    for config_name, result in successful_results.items():\n",
    "        config = ABLATION_CONFIGS[config_name]\n",
    "        \n",
    "        # Create configuration summary\n",
    "        config_summary = []\n",
    "        if config['use_images']:\n",
    "            config_summary.append(\"IMG\")\n",
    "        if config['use_tabular']:\n",
    "            config_summary.append(\"TAB\")\n",
    "        if config['use_augmentation']:\n",
    "            config_summary.append(\"AUG\")\n",
    "        if config['use_derived_features']:\n",
    "            config_summary.append(\"DER\")\n",
    "        if config['use_dropout']:\n",
    "            config_summary.append(f\"DROP({config['dropout_rate']})\")\n",
    "        if config['use_batch_norm']:\n",
    "            config_summary.append(\"BN\")\n",
    "        if config['use_pretrained']:\n",
    "            config_summary.append(\"PRE\")\n",
    "        \n",
    "        config_str = \"+\".join(config_summary)\n",
    "        \n",
    "        r2_mean, r2_lower, r2_upper = result['r2_ci']\n",
    "        mae_mean, mae_lower, mae_upper = result['mae_ci']\n",
    "        rmse_mean, rmse_lower, rmse_upper = result['rmse_ci']\n",
    "        \n",
    "        table_data.append({\n",
    "            'Configuration': config_name,\n",
    "            'Components': config_str,\n",
    "            'Total_Params': result['total_params'],\n",
    "            'Trainable_Params': result['trainable_params'],\n",
    "            'Model_Size_MB': result['model_size'],\n",
    "            'Inference_Time_ms': result['inference_time_mean'],\n",
    "            'Inference_Time_Std': result['inference_time_std'],\n",
    "            'Training_Time_s': result['training_time_mean'],\n",
    "            'Training_Time_Std': result['training_time_std'],\n",
    "            'Epochs': result['epochs_trained_mean'],\n",
    "            'R2_Mean': result['r2_mean'],\n",
    "            'R2_Std': result['r2_std'],\n",
    "            'R2_CI_Lower': r2_lower,\n",
    "            'R2_CI_Upper': r2_upper,\n",
    "            'MAE_Mean': result['mae_mean'],\n",
    "            'MAE_Std': result['mae_std'],\n",
    "            'MAE_CI_Lower': mae_lower,\n",
    "            'MAE_CI_Upper': mae_upper,\n",
    "            'RMSE_Mean': result['rmse_mean'],\n",
    "            'RMSE_Std': result['rmse_std'],\n",
    "            'RMSE_CI_Lower': rmse_lower,\n",
    "            'RMSE_CI_Upper': rmse_upper,\n",
    "            'MAPE_Mean': result['mape_mean'],\n",
    "            'MAPE_Std': result['mape_std'],\n",
    "            'Within_1_BMI_%': result['within_1_mean'],\n",
    "            'Within_2_BMI_%': result['within_2_mean'],\n",
    "            'Within_3_BMI_%': result['within_3_mean'],\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    comprehensive_table = pd.DataFrame(table_data)\n",
    "    comprehensive_table = comprehensive_table.sort_values('R2_Mean', ascending=False)\n",
    "    \n",
    "    # Save full table\n",
    "    comprehensive_table.to_csv('ablation_comprehensive_table.csv', index=False)\n",
    "    print(\"‚úÖ Comprehensive table saved to 'ablation_comprehensive_table.csv'\")\n",
    "    \n",
    "    # Create formatted display table (similar to the image)\n",
    "    display_table = pd.DataFrame({\n",
    "        'Setting': [row['Configuration'] for _, row in comprehensive_table.iterrows()],\n",
    "        'Components': [row['Components'] for _, row in comprehensive_table.iterrows()],\n",
    "        'Params (M)': [f\"{row['Total_Params']/1e6:.2f}\" for _, row in comprehensive_table.iterrows()],\n",
    "        'Size (MB)': [f\"{row['Model_Size_MB']:.1f}\" for _, row in comprehensive_table.iterrows()],\n",
    "        'Inf. Time (ms)': [f\"{row['Inference_Time_ms']:.2f}¬±{row['Inference_Time_Std']:.2f}\" \n",
    "                          for _, row in comprehensive_table.iterrows()],\n",
    "        'R¬≤ Score': [f\"{row['R2_Mean']:.4f}¬±{row['R2_Std']:.4f}\" \n",
    "                    for _, row in comprehensive_table.iterrows()],\n",
    "        'R¬≤ CI (95%)': [f\"[{row['R2_CI_Lower']:.4f}, {row['R2_CI_Upper']:.4f}]\" \n",
    "                       for _, row in comprehensive_table.iterrows()],\n",
    "        'MAE': [f\"{row['MAE_Mean']:.2f}¬±{row['MAE_Std']:.2f}\" \n",
    "               for _, row in comprehensive_table.iterrows()],\n",
    "        'RMSE': [f\"{row['RMSE_Mean']:.2f}¬±{row['RMSE_Std']:.2f}\" \n",
    "                for _, row in comprehensive_table.iterrows()],\n",
    "        'MAPE (%)': [f\"{row['MAPE_Mean']:.2f}¬±{row['MAPE_Std']:.2f}\" \n",
    "                    for _, row in comprehensive_table.iterrows()],\n",
    "        'Acc@1': [f\"{row['Within_1_BMI_%']:.1f}\" for _, row in comprehensive_table.iterrows()],\n",
    "        'Acc@2': [f\"{row['Within_2_BMI_%']:.1f}\" for _, row in comprehensive_table.iterrows()],\n",
    "        'Acc@3': [f\"{row['Within_3_BMI_%']:.1f}\" for _, row in comprehensive_table.iterrows()],\n",
    "    })\n",
    "    \n",
    "    display_table.to_csv('ablation_display_table.csv', index=False)\n",
    "    print(\"‚úÖ Display table saved to 'ablation_display_table.csv'\")\n",
    "    \n",
    "    # Print formatted table\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìã ABLATION STUDY TABLE\")\n",
    "    print(\"=\"*80)\n",
    "    print(display_table.to_string(index=False))\n",
    "    \n",
    "    # Create visualization\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. R¬≤ Comparison with error bars\n",
    "    ax1 = fig.add_subplot(gs[0, :2])\n",
    "    configs = comprehensive_table['Configuration'].values\n",
    "    r2_means = comprehensive_table['R2_Mean'].values\n",
    "    r2_stds = comprehensive_table['R2_Std'].values\n",
    "    colors = ['red' if c == 'FULL_MODEL' else 'skyblue' for c in configs]\n",
    "    \n",
    "    bars = ax1.barh(range(len(configs)), r2_means, xerr=r2_stds, \n",
    "                     color=colors, edgecolor='black', capsize=5)\n",
    "    ax1.set_yticks(range(len(configs)))\n",
    "    ax1.set_yticklabels(configs)\n",
    "    ax1.set_xlabel('R¬≤ Score', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('R¬≤ Score by Configuration (with std dev)', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    for i, (mean, std) in enumerate(zip(r2_means, r2_stds)):\n",
    "        ax1.text(mean + std + 0.01, i, f'{mean:.4f}¬±{std:.4f}', \n",
    "                va='center', fontsize=9)\n",
    "    \n",
    "    # 2. Inference Time Comparison\n",
    "    ax2 = fig.add_subplot(gs[0, 2])\n",
    "    inf_times = comprehensive_table['Inference_Time_ms'].values\n",
    "    inf_stds = comprehensive_table['Inference_Time_Std'].values\n",
    "    \n",
    "    bars = ax2.barh(range(len(configs)), inf_times, xerr=inf_stds,\n",
    "                     color='coral', edgecolor='black', capsize=5)\n",
    "    ax2.set_yticks(range(len(configs)))\n",
    "    ax2.set_yticklabels(configs)\n",
    "    ax2.set_xlabel('Inference Time (ms)', fontsize=10, fontweight='bold')\n",
    "    ax2.set_title('Inference Speed', fontsize=12, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # 3. Model Size Comparison\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    sizes = comprehensive_table['Model_Size_MB'].values\n",
    "    bars = ax3.bar(range(len(configs)), sizes, color='lightgreen', edgecolor='black')\n",
    "    ax3.set_xticks(range(len(configs)))\n",
    "    ax3.set_xticklabels(configs, rotation=45, ha='right')\n",
    "    ax3.set_ylabel('Size (MB)', fontsize=10, fontweight='bold')\n",
    "    ax3.set_title('Model Size', fontsize=12, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for i, size in enumerate(sizes):\n",
    "        ax3.text(i, size + 0.5, f'{size:.1f}', ha='center', fontsize=8)\n",
    "    \n",
    "    # 4. Parameter Count\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    params = comprehensive_table['Total_Params'].values / 1e6  # In millions\n",
    "    bars = ax4.bar(range(len(configs)), params, color='plum', edgecolor='black')\n",
    "    ax4.set_xticks(range(len(configs)))\n",
    "    ax4.set_xticklabels(configs, rotation=45, ha='right')\n",
    "    ax4.set_ylabel('Parameters (M)', fontsize=10, fontweight='bold')\n",
    "    ax4.set_title('Total Parameters', fontsize=12, fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for i, param in enumerate(params):\n",
    "        ax4.text(i, param + 0.2, f'{param:.2f}M', ha='center', fontsize=8)\n",
    "    \n",
    "    # 5. MAE with confidence intervals\n",
    "    ax5 = fig.add_subplot(gs[1, 2])\n",
    "    mae_means = comprehensive_table['MAE_Mean'].values\n",
    "    mae_lower = comprehensive_table['MAE_CI_Lower'].values\n",
    "    mae_upper = comprehensive_table['MAE_CI_Upper'].values\n",
    "    mae_err = [mae_means - mae_lower, mae_upper - mae_means]\n",
    "    \n",
    "    ax5.barh(range(len(configs)), mae_means, xerr=mae_err,\n",
    "             color='wheat', edgecolor='black', capsize=5)\n",
    "    ax5.set_yticks(range(len(configs)))\n",
    "    ax5.set_yticklabels(configs)\n",
    "    ax5.set_xlabel('MAE (kg/m¬≤)', fontsize=10, fontweight='bold')\n",
    "    ax5.set_title('MAE with 95% CI', fontsize=12, fontweight='bold')\n",
    "    ax5.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # 6. Within threshold accuracy\n",
    "    ax6 = fig.add_subplot(gs[2, :])\n",
    "    within_1 = comprehensive_table['Within_1_BMI_%'].values\n",
    "    within_2 = comprehensive_table['Within_2_BMI_%'].values\n",
    "    within_3 = comprehensive_table['Within_3_BMI_%'].values\n",
    "    \n",
    "    x = np.arange(len(configs))\n",
    "    width = 0.25\n",
    "    \n",
    "    ax6.bar(x - width, within_1, width, label='Within ¬±1 BMI', color='lightcoral')\n",
    "    ax6.bar(x, within_2, width, label='Within ¬±2 BMI', color='lightskyblue')\n",
    "    ax6.bar(x + width, within_3, width, label='Within ¬±3 BMI', color='lightgreen')\n",
    "    \n",
    "    ax6.set_xticks(x)\n",
    "    ax6.set_xticklabels(configs, rotation=45, ha='right')\n",
    "    ax6.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "    ax6.set_title('Prediction Accuracy within Thresholds', fontsize=14, fontweight='bold')\n",
    "    ax6.legend()\n",
    "    ax6.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ablation_comprehensive_results.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"‚úÖ Comprehensive visualization saved\")\n",
    "\n",
    "# ============================================================================\n",
    "# STATISTICAL ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä STATISTICAL ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(successful_results) > 1:\n",
    "    print(\"\\nüîç Performance Comparison (vs FULL_MODEL):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    if 'FULL_MODEL' in successful_results:\n",
    "        full_model = successful_results['FULL_MODEL']\n",
    "        \n",
    "        for config_name, result in successful_results.items():\n",
    "            if config_name != 'FULL_MODEL':\n",
    "                r2_diff = full_model['r2_mean'] - result['r2_mean']\n",
    "                mae_diff = result['mae_mean'] - full_model['mae_mean']\n",
    "                speed_ratio = result['inference_time_mean'] / full_model['inference_time_mean']\n",
    "                param_ratio = result['total_params'] / full_model['total_params']\n",
    "                \n",
    "                print(f\"\\n{config_name}:\")\n",
    "                print(f\"  R¬≤ Œî: {r2_diff:+.4f} ({r2_diff/full_model['r2_mean']*100:+.1f}%)\")\n",
    "                print(f\"  MAE Œî: {mae_diff:+.2f} kg/m¬≤\")\n",
    "                print(f\"  Speed: {speed_ratio:.2f}x {'faster' if speed_ratio < 1 else 'slower'}\")\n",
    "                print(f\"  Params: {param_ratio:.2f}x {'fewer' if param_ratio < 1 else 'more'}\")\n",
    "\n",
    "print(\"\\n‚úÖ Enhanced ablation study complete!\")\n",
    "print(f\"   Generated files:\")\n",
    "print(f\"   ‚Ä¢ ablation_comprehensive_table.csv\")\n",
    "print(f\"   ‚Ä¢ ablation_display_table.csv\")\n",
    "print(f\"   ‚Ä¢ ablation_comprehensive_results.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
